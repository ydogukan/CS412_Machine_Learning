{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Doğukan_Yıldırım_ML202102_HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko3omrGs8GAI"
      },
      "source": [
        "# **HOMEWORK 4**\n",
        "---\n",
        "\n",
        "**Goal:** Build a multilayer perceptron (MLP) model & for classifying SVHN dataset.\n",
        "\n",
        "**Dataset:** SVHN, 32x32 RGB number images.        \n",
        "http://ufldl.stanford.edu/housenumbers/\n",
        "\n",
        "---\n",
        "\n",
        "###**Instructions**             \n",
        "**1)** **Preprocessing**  \n",
        "> **1.1)** Load the dataset                 \n",
        "> **1.2)** Normalize features                \n",
        "  **1.3)** Visualize dataset\n",
        "\n",
        "**2)** **Modelling**\n",
        "> **2.1)** Try different fully hyperparameters (# of hidden layers, learning rate, # of epochs, # of neurons, add dropout, etc.)             \n",
        "> **2.2)** Train with early stopping\n",
        "\n",
        "**3)** **Report**                   \n",
        "Share your results, which hyperparameters you used, train & test accuracy, etc. \n",
        "Write an at most 1/2-page summary of your approach to this problem at the end of your notebook; this should be like an executive summary. Include problem definition and preprocessing as well.\n",
        "\n",
        "\n",
        "> **Topics to Discuss:**                                      \n",
        "Explain you results.           \n",
        "How did you choose the best hyperparameters?                         \n",
        "What happened when the # of epochs are too large/small, why?                  \n",
        "What happened when the learning rate is too large/small, why?              \n",
        "What did you observe when you change the # of hidden layers?                \n",
        "What did you observe when you change the # of neurons?   \n",
        "What is the use of adding dropout?                     \n",
        "What is the use of early stopping? \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Note:** Don't forget to change Colab's runtime to GPU.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1PahS-1Ez2j"
      },
      "source": [
        "#1) Preprocessing\n",
        "Load, normalize and visualize data\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzZMCgL7c80Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431a16c8-b1e8-4352-c62b-bc84af36b7d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ayvXVtjUJWZ"
      },
      "source": [
        "# Importing required modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.io import loadmat\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWoXQBAk7mhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c4ce1c-146e-4772-e3a1-97f8b00f490b"
      },
      "source": [
        "# Load SVHN dataset\n",
        "\n",
        "def load_data(path):\n",
        "    data = loadmat(path)\n",
        "    return data['X'], data['y']\n",
        "\n",
        "X_train, y_train = load_data('./drive/My Drive/svhn_dataset/train_32x32.mat')\n",
        "X_test, y_test = load_data('./drive/My Drive/svhn_dataset/test_32x32.mat')\n",
        "\n",
        "\n",
        "# Summarize dataset (count, shape, min/max value)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "print(f\"X_train max: {X_train.max()}\")\n",
        "print(f\"X_train min: {X_train.min()}\")\n",
        "print(f\"y_train unique values: {list(np.unique(y_train))}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (32, 32, 3, 73257)\n",
            "y_train shape: (73257, 1)\n",
            "X_test shape: (32, 32, 3, 26032)\n",
            "y_test shape: (26032, 1)\n",
            "X_train max: 255\n",
            "X_train min: 0\n",
            "y_train unique values: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfwHo1ZGckJU"
      },
      "source": [
        "# Change input shape\n",
        "\n",
        "X_train = np.moveaxis(X_train, -1, 0)\n",
        "X_test = np.moveaxis(X_test, -1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YawuLMTUFYWc"
      },
      "source": [
        "# Normalize the dataset\n",
        "\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# Since '0' has label 10, changing 10s to 0s\n",
        "\n",
        "y_train = np.where(y_train==10, 0, y_train)\n",
        "y_test = np.where(y_test==10, 0, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz-mQ09q_nhQ"
      },
      "source": [
        "# One-hot encoding labels\n",
        "from keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSd5ehWzGEpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "3ac7548b-8d83-4ad4-a3da-d89a7cb14f95"
      },
      "source": [
        "# Visualize some samples\n",
        "\n",
        "from random import randint\n",
        "\n",
        "fig, axes = plt.subplots(3,3, figsize=(9,9))\n",
        "\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    axes[i][j].imshow(X_train[randint(0,73256)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-cMAncVUJKr"
      },
      "source": [
        "#2) Modelling\n",
        "(Build, compile, fit, evaluate)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04IPqSXXU6pU"
      },
      "source": [
        "##2.1) Part 1\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_il-mcXkb_H1"
      },
      "source": [
        "# Importing required modules\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers import Dense, Flatten, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P056T1gORCod"
      },
      "source": [
        "### Model 1\n",
        "\n",
        "\n",
        "`learning_rate: 0.1`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Rh1vRUUS4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d921ae-6b79-4a11-8dd8-a9dae9ea1396"
      },
      "source": [
        "# Build the model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,640,330\n",
            "Trainable params: 1,640,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWjsPZwOUfpZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc69af60-f5e1-4a3e-c6fb-51b377ec1d2f"
      },
      "source": [
        "# Compile & fit the model \n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.1), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)\n",
        "\n",
        "# Hint: You can assign batch sizes 128, 256"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 3s 10ms/step - loss: 237.1289 - accuracy: 0.1561 - val_loss: 2.2376 - val_accuracy: 0.1875\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2391 - accuracy: 0.1894 - val_loss: 2.2399 - val_accuracy: 0.1875\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2386 - accuracy: 0.1894 - val_loss: 2.2389 - val_accuracy: 0.1875\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2384 - accuracy: 0.1868 - val_loss: 2.2401 - val_accuracy: 0.1875\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2382 - accuracy: 0.1893 - val_loss: 2.2413 - val_accuracy: 0.1875\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2424 - accuracy: 0.1846 - val_loss: 2.2391 - val_accuracy: 0.1875\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2388 - accuracy: 0.1919 - val_loss: 2.2444 - val_accuracy: 0.1875\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2394 - accuracy: 0.1886 - val_loss: 2.2414 - val_accuracy: 0.1875\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2431 - accuracy: 0.1857 - val_loss: 2.2416 - val_accuracy: 0.1875\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2404 - accuracy: 0.1910 - val_loss: 2.2399 - val_accuracy: 0.1875\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2407 - accuracy: 0.1883 - val_loss: 2.2405 - val_accuracy: 0.1875\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2428 - accuracy: 0.1865 - val_loss: 2.2401 - val_accuracy: 0.1875\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2432 - accuracy: 0.1858 - val_loss: 2.2404 - val_accuracy: 0.1875\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2408 - accuracy: 0.1910 - val_loss: 2.2423 - val_accuracy: 0.1875\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2408 - accuracy: 0.1821 - val_loss: 2.2396 - val_accuracy: 0.1875\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2396 - accuracy: 0.1895 - val_loss: 2.2398 - val_accuracy: 0.1875\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2406 - accuracy: 0.1898 - val_loss: 2.2384 - val_accuracy: 0.1875\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2398 - accuracy: 0.1892 - val_loss: 2.2398 - val_accuracy: 0.1875\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2426 - accuracy: 0.1858 - val_loss: 2.2427 - val_accuracy: 0.1875\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2417 - accuracy: 0.1884 - val_loss: 2.2395 - val_accuracy: 0.1875\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2405 - accuracy: 0.1896 - val_loss: 2.2383 - val_accuracy: 0.1875\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2407 - accuracy: 0.1896 - val_loss: 2.2407 - val_accuracy: 0.1875\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2411 - accuracy: 0.1887 - val_loss: 2.2410 - val_accuracy: 0.1875\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2404 - accuracy: 0.1897 - val_loss: 2.2441 - val_accuracy: 0.1875\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2438 - accuracy: 0.1877 - val_loss: 2.2448 - val_accuracy: 0.1875\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2398 - accuracy: 0.1915 - val_loss: 2.2410 - val_accuracy: 0.1875\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2393 - accuracy: 0.1910 - val_loss: 2.2424 - val_accuracy: 0.1875\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2422 - accuracy: 0.1889 - val_loss: 2.2395 - val_accuracy: 0.1875\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2402 - accuracy: 0.1902 - val_loss: 2.2430 - val_accuracy: 0.1875\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2390 - accuracy: 0.1917 - val_loss: 2.2423 - val_accuracy: 0.1875\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2426 - accuracy: 0.1884 - val_loss: 2.2403 - val_accuracy: 0.1875\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2396 - accuracy: 0.1894 - val_loss: 2.2397 - val_accuracy: 0.1875\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2402 - accuracy: 0.1889 - val_loss: 2.2407 - val_accuracy: 0.1875\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2375 - accuracy: 0.1918 - val_loss: 2.2387 - val_accuracy: 0.1875\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2367 - accuracy: 0.1921 - val_loss: 2.2379 - val_accuracy: 0.1875\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2419 - accuracy: 0.1871 - val_loss: 2.2411 - val_accuracy: 0.1875\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2349 - accuracy: 0.1940 - val_loss: 2.2394 - val_accuracy: 0.1875\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2404 - accuracy: 0.1901 - val_loss: 2.2397 - val_accuracy: 0.1875\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2422 - accuracy: 0.1879 - val_loss: 2.2388 - val_accuracy: 0.1875\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2394 - accuracy: 0.1913 - val_loss: 2.2397 - val_accuracy: 0.1875\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2383 - accuracy: 0.1892 - val_loss: 2.2399 - val_accuracy: 0.1875\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2432 - accuracy: 0.1865 - val_loss: 2.2398 - val_accuracy: 0.1875\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2422 - accuracy: 0.1875 - val_loss: 2.2396 - val_accuracy: 0.1875\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2406 - accuracy: 0.1900 - val_loss: 2.2425 - val_accuracy: 0.1875\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2417 - accuracy: 0.1920 - val_loss: 2.2441 - val_accuracy: 0.1875\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2406 - accuracy: 0.1895 - val_loss: 2.2427 - val_accuracy: 0.1875\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2403 - accuracy: 0.1902 - val_loss: 2.2427 - val_accuracy: 0.1875\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2404 - accuracy: 0.1897 - val_loss: 2.2401 - val_accuracy: 0.1875\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2389 - accuracy: 0.1883 - val_loss: 2.2386 - val_accuracy: 0.1875\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2401 - accuracy: 0.1899 - val_loss: 2.2382 - val_accuracy: 0.1875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDtYb9X0UjBl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9b8c61-2daf-4018-bd1f-68c401740f32"
      },
      "source": [
        "# Evaluate the model on test data\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 2.2242 - accuracy: 0.1959\n",
            "Test loss: 2.2242040634155273\n",
            "Test accuracy: 0.19587430357933044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_xYleMzeI4P"
      },
      "source": [
        "# Try different hyperparameters and observe the results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9O6U06uRIAJ"
      },
      "source": [
        "### Model 2\n",
        "Comparing performance according to <font color=red>`learning_rate`</font> with *Model 1*\n",
        "\n",
        "`learning_rate: 0.00001` changed from <font color=red>`0.1`</font>\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WKnhLL3MT8D",
        "outputId": "10bcd6e4-dc20-4349-9889-a5c3431a4eb0"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_1 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,640,330\n",
            "Trainable params: 1,640,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9KL7TwLQ3XA",
        "outputId": "fa249341-eeea-4ed8-9b08-b73cb21e1167"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2708 - accuracy: 0.1757 - val_loss: 2.2064 - val_accuracy: 0.2103\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.1960 - accuracy: 0.2184 - val_loss: 2.1642 - val_accuracy: 0.2413\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1486 - accuracy: 0.2478 - val_loss: 2.1118 - val_accuracy: 0.2717\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0912 - accuracy: 0.2850 - val_loss: 2.0570 - val_accuracy: 0.3130\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0322 - accuracy: 0.3182 - val_loss: 1.9998 - val_accuracy: 0.3391\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.9751 - accuracy: 0.3532 - val_loss: 1.9386 - val_accuracy: 0.3819\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.9149 - accuracy: 0.3851 - val_loss: 1.8825 - val_accuracy: 0.3934\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.8512 - accuracy: 0.4172 - val_loss: 1.8237 - val_accuracy: 0.4199\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.7915 - accuracy: 0.4424 - val_loss: 1.7679 - val_accuracy: 0.4558\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7352 - accuracy: 0.4674 - val_loss: 1.7159 - val_accuracy: 0.4713\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6866 - accuracy: 0.4909 - val_loss: 1.6733 - val_accuracy: 0.4995\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6354 - accuracy: 0.5150 - val_loss: 1.6268 - val_accuracy: 0.5229\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.5910 - accuracy: 0.5311 - val_loss: 1.5832 - val_accuracy: 0.5392\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5441 - accuracy: 0.5480 - val_loss: 1.5484 - val_accuracy: 0.5347\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.5107 - accuracy: 0.5608 - val_loss: 1.5148 - val_accuracy: 0.5573\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4733 - accuracy: 0.5769 - val_loss: 1.4757 - val_accuracy: 0.5753\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4431 - accuracy: 0.5859 - val_loss: 1.4463 - val_accuracy: 0.5784\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4100 - accuracy: 0.5965 - val_loss: 1.4203 - val_accuracy: 0.5865\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3834 - accuracy: 0.6061 - val_loss: 1.3937 - val_accuracy: 0.6178\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3551 - accuracy: 0.6209 - val_loss: 1.3679 - val_accuracy: 0.6182\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3320 - accuracy: 0.6231 - val_loss: 1.3458 - val_accuracy: 0.6277\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3050 - accuracy: 0.6322 - val_loss: 1.3215 - val_accuracy: 0.6272\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2866 - accuracy: 0.6403 - val_loss: 1.3008 - val_accuracy: 0.6390\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2656 - accuracy: 0.6429 - val_loss: 1.2835 - val_accuracy: 0.6366\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2423 - accuracy: 0.6507 - val_loss: 1.2623 - val_accuracy: 0.6467\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2242 - accuracy: 0.6550 - val_loss: 1.2466 - val_accuracy: 0.6484\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2164 - accuracy: 0.6572 - val_loss: 1.2300 - val_accuracy: 0.6603\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1938 - accuracy: 0.6677 - val_loss: 1.2170 - val_accuracy: 0.6577\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1843 - accuracy: 0.6668 - val_loss: 1.2048 - val_accuracy: 0.6646\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1677 - accuracy: 0.6694 - val_loss: 1.1918 - val_accuracy: 0.6635\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1504 - accuracy: 0.6753 - val_loss: 1.1762 - val_accuracy: 0.6676\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1383 - accuracy: 0.6828 - val_loss: 1.1693 - val_accuracy: 0.6630\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1208 - accuracy: 0.6837 - val_loss: 1.1540 - val_accuracy: 0.6736\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1152 - accuracy: 0.6856 - val_loss: 1.1474 - val_accuracy: 0.6773\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1007 - accuracy: 0.6890 - val_loss: 1.1331 - val_accuracy: 0.6755\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0908 - accuracy: 0.6935 - val_loss: 1.1221 - val_accuracy: 0.6787\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0779 - accuracy: 0.6942 - val_loss: 1.1114 - val_accuracy: 0.6855\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0717 - accuracy: 0.6957 - val_loss: 1.1013 - val_accuracy: 0.6857\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0571 - accuracy: 0.6996 - val_loss: 1.0974 - val_accuracy: 0.6920\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0511 - accuracy: 0.7059 - val_loss: 1.0866 - val_accuracy: 0.6902\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0454 - accuracy: 0.7038 - val_loss: 1.0789 - val_accuracy: 0.6883\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0432 - accuracy: 0.7032 - val_loss: 1.0706 - val_accuracy: 0.6914\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0172 - accuracy: 0.7095 - val_loss: 1.0612 - val_accuracy: 0.6945\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0111 - accuracy: 0.7104 - val_loss: 1.0521 - val_accuracy: 0.7014\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0103 - accuracy: 0.7118 - val_loss: 1.0454 - val_accuracy: 0.7009\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0013 - accuracy: 0.7156 - val_loss: 1.0428 - val_accuracy: 0.6983\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9829 - accuracy: 0.7201 - val_loss: 1.0338 - val_accuracy: 0.7048\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9868 - accuracy: 0.7175 - val_loss: 1.0250 - val_accuracy: 0.7056\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9752 - accuracy: 0.7219 - val_loss: 1.0205 - val_accuracy: 0.7066\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9739 - accuracy: 0.7224 - val_loss: 1.0186 - val_accuracy: 0.7036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvmX5HUwRAa4",
        "outputId": "28681d64-f034-4ca9-9eb8-425bd41fdb09"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 1.0896 - accuracy: 0.6794\n",
            "Test loss: 1.0896114110946655\n",
            "Test accuracy: 0.6793946027755737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BIftCepRKTb"
      },
      "source": [
        "### Model 3\n",
        "\n",
        "Comparing performance according to <font color=red>`epoch`</font> with *Model 2*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 100` changed from <font color=red>`50`</font>\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUbC6MVdVHX3",
        "outputId": "2531001e-1139-441f-91f5-ca0bdc56a692"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_2 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,640,330\n",
            "Trainable params: 1,640,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPIOU3mjVHX3",
        "outputId": "0c648ef4-d2cf-4aa6-bf2b-c863872a6e73"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=100, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2498 - accuracy: 0.1879 - val_loss: 2.2049 - val_accuracy: 0.2030\n",
            "Epoch 2/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1933 - accuracy: 0.2141 - val_loss: 2.1633 - val_accuracy: 0.2372\n",
            "Epoch 3/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.1504 - accuracy: 0.2437 - val_loss: 2.1113 - val_accuracy: 0.2566\n",
            "Epoch 4/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0944 - accuracy: 0.2774 - val_loss: 2.0523 - val_accuracy: 0.3130\n",
            "Epoch 5/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0318 - accuracy: 0.3272 - val_loss: 1.9944 - val_accuracy: 0.3542\n",
            "Epoch 6/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9646 - accuracy: 0.3596 - val_loss: 1.9288 - val_accuracy: 0.3813\n",
            "Epoch 7/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8997 - accuracy: 0.3976 - val_loss: 1.8616 - val_accuracy: 0.4192\n",
            "Epoch 8/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8291 - accuracy: 0.4340 - val_loss: 1.7996 - val_accuracy: 0.4394\n",
            "Epoch 9/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7733 - accuracy: 0.4578 - val_loss: 1.7422 - val_accuracy: 0.4729\n",
            "Epoch 10/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7156 - accuracy: 0.4872 - val_loss: 1.6907 - val_accuracy: 0.5029\n",
            "Epoch 11/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6581 - accuracy: 0.5117 - val_loss: 1.6387 - val_accuracy: 0.5278\n",
            "Epoch 12/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6033 - accuracy: 0.5331 - val_loss: 1.5935 - val_accuracy: 0.5412\n",
            "Epoch 13/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5604 - accuracy: 0.5501 - val_loss: 1.5553 - val_accuracy: 0.5723\n",
            "Epoch 14/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5291 - accuracy: 0.5642 - val_loss: 1.5135 - val_accuracy: 0.5666\n",
            "Epoch 15/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4864 - accuracy: 0.5785 - val_loss: 1.4816 - val_accuracy: 0.5715\n",
            "Epoch 16/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4442 - accuracy: 0.5930 - val_loss: 1.4500 - val_accuracy: 0.5964\n",
            "Epoch 17/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.4241 - accuracy: 0.6002 - val_loss: 1.4198 - val_accuracy: 0.5945\n",
            "Epoch 18/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3902 - accuracy: 0.6087 - val_loss: 1.3924 - val_accuracy: 0.6073\n",
            "Epoch 19/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3610 - accuracy: 0.6172 - val_loss: 1.3709 - val_accuracy: 0.6255\n",
            "Epoch 20/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3372 - accuracy: 0.6287 - val_loss: 1.3431 - val_accuracy: 0.6224\n",
            "Epoch 21/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3049 - accuracy: 0.6354 - val_loss: 1.3245 - val_accuracy: 0.6211\n",
            "Epoch 22/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2933 - accuracy: 0.6392 - val_loss: 1.3026 - val_accuracy: 0.6261\n",
            "Epoch 23/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2762 - accuracy: 0.6424 - val_loss: 1.2852 - val_accuracy: 0.6329\n",
            "Epoch 24/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2562 - accuracy: 0.6481 - val_loss: 1.2684 - val_accuracy: 0.6397\n",
            "Epoch 25/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2292 - accuracy: 0.6562 - val_loss: 1.2534 - val_accuracy: 0.6446\n",
            "Epoch 26/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2145 - accuracy: 0.6601 - val_loss: 1.2335 - val_accuracy: 0.6491\n",
            "Epoch 27/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1970 - accuracy: 0.6664 - val_loss: 1.2228 - val_accuracy: 0.6527\n",
            "Epoch 28/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1863 - accuracy: 0.6685 - val_loss: 1.2062 - val_accuracy: 0.6596\n",
            "Epoch 29/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1685 - accuracy: 0.6713 - val_loss: 1.1925 - val_accuracy: 0.6630\n",
            "Epoch 30/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1717 - accuracy: 0.6715 - val_loss: 1.1814 - val_accuracy: 0.6673\n",
            "Epoch 31/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1426 - accuracy: 0.6816 - val_loss: 1.1671 - val_accuracy: 0.6709\n",
            "Epoch 32/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1329 - accuracy: 0.6812 - val_loss: 1.1558 - val_accuracy: 0.6766\n",
            "Epoch 33/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1263 - accuracy: 0.6832 - val_loss: 1.1482 - val_accuracy: 0.6755\n",
            "Epoch 34/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1135 - accuracy: 0.6870 - val_loss: 1.1365 - val_accuracy: 0.6773\n",
            "Epoch 35/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0987 - accuracy: 0.6904 - val_loss: 1.1273 - val_accuracy: 0.6773\n",
            "Epoch 36/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0908 - accuracy: 0.6902 - val_loss: 1.1174 - val_accuracy: 0.6848\n",
            "Epoch 37/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0792 - accuracy: 0.6961 - val_loss: 1.1112 - val_accuracy: 0.6833\n",
            "Epoch 38/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0727 - accuracy: 0.6963 - val_loss: 1.1052 - val_accuracy: 0.6840\n",
            "Epoch 39/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0576 - accuracy: 0.6999 - val_loss: 1.0931 - val_accuracy: 0.6882\n",
            "Epoch 40/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0490 - accuracy: 0.7024 - val_loss: 1.0830 - val_accuracy: 0.6895\n",
            "Epoch 41/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0469 - accuracy: 0.7022 - val_loss: 1.0790 - val_accuracy: 0.6907\n",
            "Epoch 42/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0384 - accuracy: 0.7065 - val_loss: 1.0660 - val_accuracy: 0.6967\n",
            "Epoch 43/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0293 - accuracy: 0.7076 - val_loss: 1.0631 - val_accuracy: 0.6979\n",
            "Epoch 44/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0259 - accuracy: 0.7084 - val_loss: 1.0566 - val_accuracy: 0.6967\n",
            "Epoch 45/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0123 - accuracy: 0.7130 - val_loss: 1.0476 - val_accuracy: 0.6986\n",
            "Epoch 46/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0157 - accuracy: 0.7116 - val_loss: 1.0505 - val_accuracy: 0.6925\n",
            "Epoch 47/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9969 - accuracy: 0.7175 - val_loss: 1.0343 - val_accuracy: 0.7009\n",
            "Epoch 48/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9960 - accuracy: 0.7157 - val_loss: 1.0285 - val_accuracy: 0.7016\n",
            "Epoch 49/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9831 - accuracy: 0.7193 - val_loss: 1.0225 - val_accuracy: 0.7052\n",
            "Epoch 50/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9893 - accuracy: 0.7146 - val_loss: 1.0215 - val_accuracy: 0.6995\n",
            "Epoch 51/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9707 - accuracy: 0.7235 - val_loss: 1.0130 - val_accuracy: 0.7041\n",
            "Epoch 52/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9720 - accuracy: 0.7219 - val_loss: 1.0151 - val_accuracy: 0.7018\n",
            "Epoch 53/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9673 - accuracy: 0.7260 - val_loss: 1.0067 - val_accuracy: 0.7071\n",
            "Epoch 54/100\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9559 - accuracy: 0.7268 - val_loss: 0.9950 - val_accuracy: 0.7130\n",
            "Epoch 55/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9524 - accuracy: 0.7281 - val_loss: 0.9924 - val_accuracy: 0.7099\n",
            "Epoch 56/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9443 - accuracy: 0.7310 - val_loss: 0.9896 - val_accuracy: 0.7125\n",
            "Epoch 57/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9402 - accuracy: 0.7303 - val_loss: 0.9821 - val_accuracy: 0.7124\n",
            "Epoch 58/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9408 - accuracy: 0.7306 - val_loss: 0.9814 - val_accuracy: 0.7152\n",
            "Epoch 59/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9344 - accuracy: 0.7342 - val_loss: 0.9740 - val_accuracy: 0.7153\n",
            "Epoch 60/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9345 - accuracy: 0.7333 - val_loss: 0.9694 - val_accuracy: 0.7159\n",
            "Epoch 61/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9201 - accuracy: 0.7361 - val_loss: 0.9667 - val_accuracy: 0.7170\n",
            "Epoch 62/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9191 - accuracy: 0.7375 - val_loss: 0.9601 - val_accuracy: 0.7166\n",
            "Epoch 63/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9080 - accuracy: 0.7392 - val_loss: 0.9579 - val_accuracy: 0.7197\n",
            "Epoch 64/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9052 - accuracy: 0.7403 - val_loss: 0.9510 - val_accuracy: 0.7224\n",
            "Epoch 65/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9041 - accuracy: 0.7392 - val_loss: 0.9480 - val_accuracy: 0.7203\n",
            "Epoch 66/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8941 - accuracy: 0.7438 - val_loss: 0.9454 - val_accuracy: 0.7207\n",
            "Epoch 67/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8994 - accuracy: 0.7411 - val_loss: 0.9465 - val_accuracy: 0.7206\n",
            "Epoch 68/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8903 - accuracy: 0.7443 - val_loss: 0.9369 - val_accuracy: 0.7256\n",
            "Epoch 69/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8943 - accuracy: 0.7422 - val_loss: 0.9345 - val_accuracy: 0.7247\n",
            "Epoch 70/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8838 - accuracy: 0.7456 - val_loss: 0.9287 - val_accuracy: 0.7258\n",
            "Epoch 71/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8753 - accuracy: 0.7490 - val_loss: 0.9283 - val_accuracy: 0.7243\n",
            "Epoch 72/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8736 - accuracy: 0.7496 - val_loss: 0.9222 - val_accuracy: 0.7282\n",
            "Epoch 73/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8629 - accuracy: 0.7539 - val_loss: 0.9185 - val_accuracy: 0.7285\n",
            "Epoch 74/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8688 - accuracy: 0.7491 - val_loss: 0.9159 - val_accuracy: 0.7293\n",
            "Epoch 75/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8641 - accuracy: 0.7518 - val_loss: 0.9237 - val_accuracy: 0.7289\n",
            "Epoch 76/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8665 - accuracy: 0.7502 - val_loss: 0.9147 - val_accuracy: 0.7319\n",
            "Epoch 77/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8668 - accuracy: 0.7494 - val_loss: 0.9059 - val_accuracy: 0.7299\n",
            "Epoch 78/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8604 - accuracy: 0.7539 - val_loss: 0.9029 - val_accuracy: 0.7318\n",
            "Epoch 79/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8596 - accuracy: 0.7524 - val_loss: 0.8992 - val_accuracy: 0.7333\n",
            "Epoch 80/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8428 - accuracy: 0.7551 - val_loss: 0.8942 - val_accuracy: 0.7340\n",
            "Epoch 81/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8457 - accuracy: 0.7576 - val_loss: 0.8907 - val_accuracy: 0.7365\n",
            "Epoch 82/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8399 - accuracy: 0.7564 - val_loss: 0.8891 - val_accuracy: 0.7409\n",
            "Epoch 83/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8348 - accuracy: 0.7594 - val_loss: 0.8858 - val_accuracy: 0.7379\n",
            "Epoch 84/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8413 - accuracy: 0.7583 - val_loss: 0.8852 - val_accuracy: 0.7366\n",
            "Epoch 85/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8278 - accuracy: 0.7595 - val_loss: 0.8806 - val_accuracy: 0.7392\n",
            "Epoch 86/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8244 - accuracy: 0.7630 - val_loss: 0.8775 - val_accuracy: 0.7397\n",
            "Epoch 87/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8248 - accuracy: 0.7602 - val_loss: 0.8762 - val_accuracy: 0.7389\n",
            "Epoch 88/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8090 - accuracy: 0.7641 - val_loss: 0.8712 - val_accuracy: 0.7423\n",
            "Epoch 89/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8128 - accuracy: 0.7641 - val_loss: 0.8676 - val_accuracy: 0.7443\n",
            "Epoch 90/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8108 - accuracy: 0.7655 - val_loss: 0.8645 - val_accuracy: 0.7447\n",
            "Epoch 91/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8109 - accuracy: 0.7654 - val_loss: 0.8616 - val_accuracy: 0.7469\n",
            "Epoch 92/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8088 - accuracy: 0.7642 - val_loss: 0.8658 - val_accuracy: 0.7447\n",
            "Epoch 93/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8055 - accuracy: 0.7674 - val_loss: 0.8602 - val_accuracy: 0.7459\n",
            "Epoch 94/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7963 - accuracy: 0.7707 - val_loss: 0.8524 - val_accuracy: 0.7480\n",
            "Epoch 95/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7995 - accuracy: 0.7698 - val_loss: 0.8497 - val_accuracy: 0.7490\n",
            "Epoch 96/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7929 - accuracy: 0.7711 - val_loss: 0.8460 - val_accuracy: 0.7489\n",
            "Epoch 97/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7841 - accuracy: 0.7718 - val_loss: 0.8430 - val_accuracy: 0.7520\n",
            "Epoch 98/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7875 - accuracy: 0.7737 - val_loss: 0.8438 - val_accuracy: 0.7501\n",
            "Epoch 99/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7755 - accuracy: 0.7763 - val_loss: 0.8447 - val_accuracy: 0.7490\n",
            "Epoch 100/100\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7816 - accuracy: 0.7764 - val_loss: 0.8358 - val_accuracy: 0.7529\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLgRyyp_VHX4",
        "outputId": "65cda774-573e-4e98-8eb6-3d70db232614"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 0.9116 - accuracy: 0.7339\n",
            "Test loss: 0.9115703105926514\n",
            "Test accuracy: 0.7339044213294983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z75AoW_UU04J"
      },
      "source": [
        "### Model 4\n",
        "\n",
        "Comparing performance according to <font color = red>`epoch`</font> with *Model 2* and *Model 3*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 200` changed from <font color=red>`50 and 100`</font>\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-2ZSrsVZzSd",
        "outputId": "7002fa24-e4df-4fcb-a4a4-4f5efe6a7e35"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_3 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,640,330\n",
            "Trainable params: 1,640,330\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuvPHJ9eZzSe",
        "outputId": "5410e7d3-fd5c-4e7c-ec87-9bb7acf0719f"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=200, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2632 - accuracy: 0.1797 - val_loss: 2.1946 - val_accuracy: 0.2072\n",
            "Epoch 2/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1832 - accuracy: 0.2267 - val_loss: 2.1456 - val_accuracy: 0.2421\n",
            "Epoch 3/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1293 - accuracy: 0.2516 - val_loss: 2.0892 - val_accuracy: 0.2775\n",
            "Epoch 4/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0736 - accuracy: 0.2907 - val_loss: 2.0304 - val_accuracy: 0.3160\n",
            "Epoch 5/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0103 - accuracy: 0.3270 - val_loss: 1.9678 - val_accuracy: 0.3603\n",
            "Epoch 6/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9469 - accuracy: 0.3646 - val_loss: 1.9136 - val_accuracy: 0.3815\n",
            "Epoch 7/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8843 - accuracy: 0.3999 - val_loss: 1.8492 - val_accuracy: 0.4141\n",
            "Epoch 8/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8246 - accuracy: 0.4256 - val_loss: 1.7926 - val_accuracy: 0.4428\n",
            "Epoch 9/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7648 - accuracy: 0.4552 - val_loss: 1.7398 - val_accuracy: 0.4646\n",
            "Epoch 10/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7093 - accuracy: 0.4794 - val_loss: 1.6900 - val_accuracy: 0.4857\n",
            "Epoch 11/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.6657 - accuracy: 0.5009 - val_loss: 1.6421 - val_accuracy: 0.4970\n",
            "Epoch 12/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.6167 - accuracy: 0.5195 - val_loss: 1.6023 - val_accuracy: 0.5183\n",
            "Epoch 13/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5706 - accuracy: 0.5390 - val_loss: 1.5620 - val_accuracy: 0.5301\n",
            "Epoch 14/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5324 - accuracy: 0.5545 - val_loss: 1.5256 - val_accuracy: 0.5545\n",
            "Epoch 15/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4952 - accuracy: 0.5681 - val_loss: 1.4914 - val_accuracy: 0.5612\n",
            "Epoch 16/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4627 - accuracy: 0.5800 - val_loss: 1.4626 - val_accuracy: 0.5760\n",
            "Epoch 17/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4335 - accuracy: 0.5901 - val_loss: 1.4335 - val_accuracy: 0.5832\n",
            "Epoch 18/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4005 - accuracy: 0.5996 - val_loss: 1.4068 - val_accuracy: 0.6040\n",
            "Epoch 19/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3810 - accuracy: 0.6078 - val_loss: 1.3825 - val_accuracy: 0.6021\n",
            "Epoch 20/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3526 - accuracy: 0.6179 - val_loss: 1.3572 - val_accuracy: 0.6228\n",
            "Epoch 21/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3250 - accuracy: 0.6291 - val_loss: 1.3383 - val_accuracy: 0.6308\n",
            "Epoch 22/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3052 - accuracy: 0.6326 - val_loss: 1.3152 - val_accuracy: 0.6351\n",
            "Epoch 23/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2847 - accuracy: 0.6368 - val_loss: 1.3002 - val_accuracy: 0.6220\n",
            "Epoch 24/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2672 - accuracy: 0.6411 - val_loss: 1.2784 - val_accuracy: 0.6403\n",
            "Epoch 25/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2441 - accuracy: 0.6530 - val_loss: 1.2633 - val_accuracy: 0.6453\n",
            "Epoch 26/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2308 - accuracy: 0.6537 - val_loss: 1.2446 - val_accuracy: 0.6518\n",
            "Epoch 27/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2117 - accuracy: 0.6628 - val_loss: 1.2308 - val_accuracy: 0.6504\n",
            "Epoch 28/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2010 - accuracy: 0.6632 - val_loss: 1.2157 - val_accuracy: 0.6584\n",
            "Epoch 29/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1804 - accuracy: 0.6712 - val_loss: 1.2084 - val_accuracy: 0.6562\n",
            "Epoch 30/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1716 - accuracy: 0.6726 - val_loss: 1.1939 - val_accuracy: 0.6629\n",
            "Epoch 31/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1554 - accuracy: 0.6761 - val_loss: 1.1795 - val_accuracy: 0.6639\n",
            "Epoch 32/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1404 - accuracy: 0.6804 - val_loss: 1.1673 - val_accuracy: 0.6691\n",
            "Epoch 33/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1275 - accuracy: 0.6867 - val_loss: 1.1546 - val_accuracy: 0.6768\n",
            "Epoch 34/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1201 - accuracy: 0.6878 - val_loss: 1.1416 - val_accuracy: 0.6794\n",
            "Epoch 35/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1087 - accuracy: 0.6880 - val_loss: 1.1346 - val_accuracy: 0.6764\n",
            "Epoch 36/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1011 - accuracy: 0.6899 - val_loss: 1.1230 - val_accuracy: 0.6800\n",
            "Epoch 37/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0897 - accuracy: 0.6922 - val_loss: 1.1159 - val_accuracy: 0.6857\n",
            "Epoch 38/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0823 - accuracy: 0.6949 - val_loss: 1.1047 - val_accuracy: 0.6889\n",
            "Epoch 39/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0616 - accuracy: 0.7020 - val_loss: 1.1001 - val_accuracy: 0.6848\n",
            "Epoch 40/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0577 - accuracy: 0.6998 - val_loss: 1.0884 - val_accuracy: 0.6897\n",
            "Epoch 41/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0489 - accuracy: 0.7045 - val_loss: 1.0793 - val_accuracy: 0.6921\n",
            "Epoch 42/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0384 - accuracy: 0.7064 - val_loss: 1.0719 - val_accuracy: 0.6920\n",
            "Epoch 43/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0317 - accuracy: 0.7077 - val_loss: 1.0637 - val_accuracy: 0.6961\n",
            "Epoch 44/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0240 - accuracy: 0.7102 - val_loss: 1.0585 - val_accuracy: 0.6951\n",
            "Epoch 45/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0142 - accuracy: 0.7128 - val_loss: 1.0495 - val_accuracy: 0.6992\n",
            "Epoch 46/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0088 - accuracy: 0.7149 - val_loss: 1.0457 - val_accuracy: 0.6987\n",
            "Epoch 47/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0019 - accuracy: 0.7187 - val_loss: 1.0370 - val_accuracy: 0.7005\n",
            "Epoch 48/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.0001 - accuracy: 0.7144 - val_loss: 1.0286 - val_accuracy: 0.7056\n",
            "Epoch 49/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9943 - accuracy: 0.7176 - val_loss: 1.0247 - val_accuracy: 0.7033\n",
            "Epoch 50/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9856 - accuracy: 0.7215 - val_loss: 1.0222 - val_accuracy: 0.7060\n",
            "Epoch 51/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9754 - accuracy: 0.7207 - val_loss: 1.0143 - val_accuracy: 0.7084\n",
            "Epoch 52/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9696 - accuracy: 0.7235 - val_loss: 1.0083 - val_accuracy: 0.7110\n",
            "Epoch 53/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9758 - accuracy: 0.7220 - val_loss: 1.0059 - val_accuracy: 0.7082\n",
            "Epoch 54/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9588 - accuracy: 0.7265 - val_loss: 0.9971 - val_accuracy: 0.7129\n",
            "Epoch 55/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9601 - accuracy: 0.7255 - val_loss: 0.9911 - val_accuracy: 0.7140\n",
            "Epoch 56/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9486 - accuracy: 0.7303 - val_loss: 0.9892 - val_accuracy: 0.7113\n",
            "Epoch 57/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9510 - accuracy: 0.7272 - val_loss: 0.9815 - val_accuracy: 0.7181\n",
            "Epoch 58/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9341 - accuracy: 0.7339 - val_loss: 0.9748 - val_accuracy: 0.7164\n",
            "Epoch 59/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9331 - accuracy: 0.7314 - val_loss: 0.9723 - val_accuracy: 0.7158\n",
            "Epoch 60/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9287 - accuracy: 0.7341 - val_loss: 0.9744 - val_accuracy: 0.7172\n",
            "Epoch 61/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9251 - accuracy: 0.7330 - val_loss: 0.9647 - val_accuracy: 0.7184\n",
            "Epoch 62/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9089 - accuracy: 0.7391 - val_loss: 0.9619 - val_accuracy: 0.7200\n",
            "Epoch 63/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9141 - accuracy: 0.7369 - val_loss: 0.9598 - val_accuracy: 0.7182\n",
            "Epoch 64/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9128 - accuracy: 0.7389 - val_loss: 0.9504 - val_accuracy: 0.7207\n",
            "Epoch 65/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.9121 - accuracy: 0.7371 - val_loss: 0.9453 - val_accuracy: 0.7241\n",
            "Epoch 66/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8928 - accuracy: 0.7451 - val_loss: 0.9446 - val_accuracy: 0.7216\n",
            "Epoch 67/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8866 - accuracy: 0.7463 - val_loss: 0.9409 - val_accuracy: 0.7256\n",
            "Epoch 68/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8919 - accuracy: 0.7434 - val_loss: 0.9348 - val_accuracy: 0.7262\n",
            "Epoch 69/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8890 - accuracy: 0.7441 - val_loss: 0.9290 - val_accuracy: 0.7269\n",
            "Epoch 70/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8767 - accuracy: 0.7507 - val_loss: 0.9295 - val_accuracy: 0.7267\n",
            "Epoch 71/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8789 - accuracy: 0.7456 - val_loss: 0.9207 - val_accuracy: 0.7289\n",
            "Epoch 72/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8755 - accuracy: 0.7463 - val_loss: 0.9220 - val_accuracy: 0.7269\n",
            "Epoch 73/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8720 - accuracy: 0.7496 - val_loss: 0.9128 - val_accuracy: 0.7323\n",
            "Epoch 74/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8650 - accuracy: 0.7501 - val_loss: 0.9106 - val_accuracy: 0.7319\n",
            "Epoch 75/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8630 - accuracy: 0.7501 - val_loss: 0.9062 - val_accuracy: 0.7335\n",
            "Epoch 76/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8559 - accuracy: 0.7515 - val_loss: 0.9048 - val_accuracy: 0.7333\n",
            "Epoch 77/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8533 - accuracy: 0.7548 - val_loss: 0.8993 - val_accuracy: 0.7357\n",
            "Epoch 78/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8517 - accuracy: 0.7538 - val_loss: 0.9000 - val_accuracy: 0.7339\n",
            "Epoch 79/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8395 - accuracy: 0.7565 - val_loss: 0.9035 - val_accuracy: 0.7387\n",
            "Epoch 80/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8427 - accuracy: 0.7584 - val_loss: 0.8897 - val_accuracy: 0.7385\n",
            "Epoch 81/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8366 - accuracy: 0.7597 - val_loss: 0.8889 - val_accuracy: 0.7342\n",
            "Epoch 82/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8340 - accuracy: 0.7621 - val_loss: 0.8837 - val_accuracy: 0.7406\n",
            "Epoch 83/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8327 - accuracy: 0.7601 - val_loss: 0.8809 - val_accuracy: 0.7410\n",
            "Epoch 84/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8286 - accuracy: 0.7594 - val_loss: 0.8790 - val_accuracy: 0.7398\n",
            "Epoch 85/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8227 - accuracy: 0.7627 - val_loss: 0.8725 - val_accuracy: 0.7423\n",
            "Epoch 86/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8194 - accuracy: 0.7622 - val_loss: 0.8699 - val_accuracy: 0.7410\n",
            "Epoch 87/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8104 - accuracy: 0.7634 - val_loss: 0.8672 - val_accuracy: 0.7438\n",
            "Epoch 88/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8121 - accuracy: 0.7649 - val_loss: 0.8672 - val_accuracy: 0.7474\n",
            "Epoch 89/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8106 - accuracy: 0.7670 - val_loss: 0.8611 - val_accuracy: 0.7444\n",
            "Epoch 90/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8065 - accuracy: 0.7670 - val_loss: 0.8620 - val_accuracy: 0.7445\n",
            "Epoch 91/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.8007 - accuracy: 0.7663 - val_loss: 0.8557 - val_accuracy: 0.7475\n",
            "Epoch 92/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7972 - accuracy: 0.7705 - val_loss: 0.8565 - val_accuracy: 0.7455\n",
            "Epoch 93/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7959 - accuracy: 0.7691 - val_loss: 0.8518 - val_accuracy: 0.7473\n",
            "Epoch 94/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7950 - accuracy: 0.7689 - val_loss: 0.8474 - val_accuracy: 0.7510\n",
            "Epoch 95/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7933 - accuracy: 0.7700 - val_loss: 0.8450 - val_accuracy: 0.7510\n",
            "Epoch 96/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7895 - accuracy: 0.7698 - val_loss: 0.8407 - val_accuracy: 0.7530\n",
            "Epoch 97/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7822 - accuracy: 0.7733 - val_loss: 0.8381 - val_accuracy: 0.7533\n",
            "Epoch 98/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7769 - accuracy: 0.7761 - val_loss: 0.8451 - val_accuracy: 0.7462\n",
            "Epoch 99/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7781 - accuracy: 0.7723 - val_loss: 0.8370 - val_accuracy: 0.7492\n",
            "Epoch 100/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7661 - accuracy: 0.7784 - val_loss: 0.8302 - val_accuracy: 0.7542\n",
            "Epoch 101/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7724 - accuracy: 0.7752 - val_loss: 0.8273 - val_accuracy: 0.7553\n",
            "Epoch 102/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7653 - accuracy: 0.7782 - val_loss: 0.8248 - val_accuracy: 0.7563\n",
            "Epoch 103/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7613 - accuracy: 0.7799 - val_loss: 0.8227 - val_accuracy: 0.7574\n",
            "Epoch 104/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7690 - accuracy: 0.7789 - val_loss: 0.8285 - val_accuracy: 0.7561\n",
            "Epoch 105/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7617 - accuracy: 0.7792 - val_loss: 0.8194 - val_accuracy: 0.7586\n",
            "Epoch 106/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7530 - accuracy: 0.7825 - val_loss: 0.8216 - val_accuracy: 0.7598\n",
            "Epoch 107/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7595 - accuracy: 0.7779 - val_loss: 0.8126 - val_accuracy: 0.7609\n",
            "Epoch 108/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.7528 - accuracy: 0.7840 - val_loss: 0.8130 - val_accuracy: 0.7599\n",
            "Epoch 109/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7486 - accuracy: 0.7837 - val_loss: 0.8156 - val_accuracy: 0.7570\n",
            "Epoch 110/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7494 - accuracy: 0.7813 - val_loss: 0.8089 - val_accuracy: 0.7574\n",
            "Epoch 111/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7426 - accuracy: 0.7854 - val_loss: 0.8048 - val_accuracy: 0.7650\n",
            "Epoch 112/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7419 - accuracy: 0.7872 - val_loss: 0.8021 - val_accuracy: 0.7632\n",
            "Epoch 113/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7423 - accuracy: 0.7858 - val_loss: 0.8026 - val_accuracy: 0.7633\n",
            "Epoch 114/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7329 - accuracy: 0.7874 - val_loss: 0.7994 - val_accuracy: 0.7643\n",
            "Epoch 115/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7314 - accuracy: 0.7906 - val_loss: 0.7944 - val_accuracy: 0.7673\n",
            "Epoch 116/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7340 - accuracy: 0.7882 - val_loss: 0.7933 - val_accuracy: 0.7678\n",
            "Epoch 117/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7305 - accuracy: 0.7895 - val_loss: 0.7889 - val_accuracy: 0.7688\n",
            "Epoch 118/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7229 - accuracy: 0.7916 - val_loss: 0.7975 - val_accuracy: 0.7605\n",
            "Epoch 119/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7240 - accuracy: 0.7914 - val_loss: 0.7856 - val_accuracy: 0.7691\n",
            "Epoch 120/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7148 - accuracy: 0.7931 - val_loss: 0.7845 - val_accuracy: 0.7731\n",
            "Epoch 121/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7123 - accuracy: 0.7954 - val_loss: 0.7839 - val_accuracy: 0.7696\n",
            "Epoch 122/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7197 - accuracy: 0.7922 - val_loss: 0.7790 - val_accuracy: 0.7730\n",
            "Epoch 123/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7270 - accuracy: 0.7904 - val_loss: 0.7805 - val_accuracy: 0.7751\n",
            "Epoch 124/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7175 - accuracy: 0.7892 - val_loss: 0.7778 - val_accuracy: 0.7716\n",
            "Epoch 125/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7055 - accuracy: 0.7949 - val_loss: 0.7729 - val_accuracy: 0.7758\n",
            "Epoch 126/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7031 - accuracy: 0.7982 - val_loss: 0.7743 - val_accuracy: 0.7751\n",
            "Epoch 127/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7038 - accuracy: 0.7950 - val_loss: 0.7713 - val_accuracy: 0.7748\n",
            "Epoch 128/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.7022 - accuracy: 0.7967 - val_loss: 0.7663 - val_accuracy: 0.7749\n",
            "Epoch 129/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6937 - accuracy: 0.7994 - val_loss: 0.7713 - val_accuracy: 0.7712\n",
            "Epoch 130/200\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 0.6977 - accuracy: 0.7965 - val_loss: 0.7632 - val_accuracy: 0.7796\n",
            "Epoch 131/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6889 - accuracy: 0.8017 - val_loss: 0.7617 - val_accuracy: 0.7761\n",
            "Epoch 132/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6989 - accuracy: 0.7993 - val_loss: 0.7634 - val_accuracy: 0.7731\n",
            "Epoch 133/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6884 - accuracy: 0.7999 - val_loss: 0.7624 - val_accuracy: 0.7768\n",
            "Epoch 134/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6919 - accuracy: 0.8002 - val_loss: 0.7557 - val_accuracy: 0.7828\n",
            "Epoch 135/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6854 - accuracy: 0.8015 - val_loss: 0.7568 - val_accuracy: 0.7807\n",
            "Epoch 136/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6881 - accuracy: 0.8002 - val_loss: 0.7535 - val_accuracy: 0.7818\n",
            "Epoch 137/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6797 - accuracy: 0.8049 - val_loss: 0.7494 - val_accuracy: 0.7830\n",
            "Epoch 138/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6754 - accuracy: 0.8049 - val_loss: 0.7544 - val_accuracy: 0.7772\n",
            "Epoch 139/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6767 - accuracy: 0.8045 - val_loss: 0.7520 - val_accuracy: 0.7810\n",
            "Epoch 140/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6818 - accuracy: 0.8027 - val_loss: 0.7469 - val_accuracy: 0.7821\n",
            "Epoch 141/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6726 - accuracy: 0.8039 - val_loss: 0.7447 - val_accuracy: 0.7828\n",
            "Epoch 142/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6677 - accuracy: 0.8063 - val_loss: 0.7410 - val_accuracy: 0.7855\n",
            "Epoch 143/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6774 - accuracy: 0.8052 - val_loss: 0.7441 - val_accuracy: 0.7831\n",
            "Epoch 144/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6641 - accuracy: 0.8094 - val_loss: 0.7403 - val_accuracy: 0.7877\n",
            "Epoch 145/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6658 - accuracy: 0.8071 - val_loss: 0.7389 - val_accuracy: 0.7837\n",
            "Epoch 146/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6577 - accuracy: 0.8122 - val_loss: 0.7348 - val_accuracy: 0.7854\n",
            "Epoch 147/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6589 - accuracy: 0.8083 - val_loss: 0.7369 - val_accuracy: 0.7856\n",
            "Epoch 148/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6642 - accuracy: 0.8090 - val_loss: 0.7332 - val_accuracy: 0.7893\n",
            "Epoch 149/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6596 - accuracy: 0.8098 - val_loss: 0.7312 - val_accuracy: 0.7909\n",
            "Epoch 150/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6591 - accuracy: 0.8113 - val_loss: 0.7324 - val_accuracy: 0.7862\n",
            "Epoch 151/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6512 - accuracy: 0.8114 - val_loss: 0.7293 - val_accuracy: 0.7892\n",
            "Epoch 152/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6485 - accuracy: 0.8134 - val_loss: 0.7288 - val_accuracy: 0.7882\n",
            "Epoch 153/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6482 - accuracy: 0.8146 - val_loss: 0.7263 - val_accuracy: 0.7890\n",
            "Epoch 154/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6526 - accuracy: 0.8103 - val_loss: 0.7230 - val_accuracy: 0.7916\n",
            "Epoch 155/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6457 - accuracy: 0.8131 - val_loss: 0.7261 - val_accuracy: 0.7897\n",
            "Epoch 156/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6434 - accuracy: 0.8149 - val_loss: 0.7220 - val_accuracy: 0.7882\n",
            "Epoch 157/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6465 - accuracy: 0.8131 - val_loss: 0.7181 - val_accuracy: 0.7922\n",
            "Epoch 158/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6428 - accuracy: 0.8159 - val_loss: 0.7205 - val_accuracy: 0.7952\n",
            "Epoch 159/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6392 - accuracy: 0.8172 - val_loss: 0.7152 - val_accuracy: 0.7944\n",
            "Epoch 160/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6329 - accuracy: 0.8194 - val_loss: 0.7149 - val_accuracy: 0.7967\n",
            "Epoch 161/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6398 - accuracy: 0.8178 - val_loss: 0.7122 - val_accuracy: 0.7930\n",
            "Epoch 162/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6342 - accuracy: 0.8180 - val_loss: 0.7141 - val_accuracy: 0.7976\n",
            "Epoch 163/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6250 - accuracy: 0.8213 - val_loss: 0.7122 - val_accuracy: 0.7918\n",
            "Epoch 164/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6351 - accuracy: 0.8173 - val_loss: 0.7080 - val_accuracy: 0.7957\n",
            "Epoch 165/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6248 - accuracy: 0.8236 - val_loss: 0.7049 - val_accuracy: 0.7978\n",
            "Epoch 166/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6281 - accuracy: 0.8192 - val_loss: 0.7101 - val_accuracy: 0.7931\n",
            "Epoch 167/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6231 - accuracy: 0.8203 - val_loss: 0.7079 - val_accuracy: 0.7937\n",
            "Epoch 168/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6332 - accuracy: 0.8175 - val_loss: 0.7037 - val_accuracy: 0.7978\n",
            "Epoch 169/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6193 - accuracy: 0.8204 - val_loss: 0.7067 - val_accuracy: 0.7939\n",
            "Epoch 170/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6254 - accuracy: 0.8215 - val_loss: 0.7031 - val_accuracy: 0.7968\n",
            "Epoch 171/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6208 - accuracy: 0.8216 - val_loss: 0.7022 - val_accuracy: 0.7968\n",
            "Epoch 172/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6118 - accuracy: 0.8271 - val_loss: 0.6957 - val_accuracy: 0.8002\n",
            "Epoch 173/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6118 - accuracy: 0.8240 - val_loss: 0.6977 - val_accuracy: 0.7974\n",
            "Epoch 174/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6089 - accuracy: 0.8271 - val_loss: 0.6995 - val_accuracy: 0.8004\n",
            "Epoch 175/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6164 - accuracy: 0.8217 - val_loss: 0.6940 - val_accuracy: 0.8008\n",
            "Epoch 176/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6128 - accuracy: 0.8230 - val_loss: 0.6934 - val_accuracy: 0.7991\n",
            "Epoch 177/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6052 - accuracy: 0.8257 - val_loss: 0.7016 - val_accuracy: 0.7962\n",
            "Epoch 178/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6087 - accuracy: 0.8251 - val_loss: 0.6906 - val_accuracy: 0.8000\n",
            "Epoch 179/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6060 - accuracy: 0.8264 - val_loss: 0.6898 - val_accuracy: 0.8020\n",
            "Epoch 180/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6089 - accuracy: 0.8245 - val_loss: 0.6920 - val_accuracy: 0.8008\n",
            "Epoch 181/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6019 - accuracy: 0.8260 - val_loss: 0.6849 - val_accuracy: 0.8046\n",
            "Epoch 182/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5992 - accuracy: 0.8299 - val_loss: 0.6859 - val_accuracy: 0.8034\n",
            "Epoch 183/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6006 - accuracy: 0.8272 - val_loss: 0.6865 - val_accuracy: 0.8018\n",
            "Epoch 184/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5960 - accuracy: 0.8269 - val_loss: 0.6851 - val_accuracy: 0.8049\n",
            "Epoch 185/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6051 - accuracy: 0.8268 - val_loss: 0.6811 - val_accuracy: 0.8066\n",
            "Epoch 186/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5961 - accuracy: 0.8307 - val_loss: 0.6820 - val_accuracy: 0.8047\n",
            "Epoch 187/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.6030 - accuracy: 0.8261 - val_loss: 0.6839 - val_accuracy: 0.8043\n",
            "Epoch 188/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5989 - accuracy: 0.8278 - val_loss: 0.6819 - val_accuracy: 0.8032\n",
            "Epoch 189/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5962 - accuracy: 0.8294 - val_loss: 0.6769 - val_accuracy: 0.8061\n",
            "Epoch 190/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5884 - accuracy: 0.8310 - val_loss: 0.6755 - val_accuracy: 0.8074\n",
            "Epoch 191/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5925 - accuracy: 0.8302 - val_loss: 0.6757 - val_accuracy: 0.8057\n",
            "Epoch 192/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5820 - accuracy: 0.8343 - val_loss: 0.6771 - val_accuracy: 0.8049\n",
            "Epoch 193/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5876 - accuracy: 0.8318 - val_loss: 0.6743 - val_accuracy: 0.8065\n",
            "Epoch 194/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5894 - accuracy: 0.8298 - val_loss: 0.6713 - val_accuracy: 0.8073\n",
            "Epoch 195/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5874 - accuracy: 0.8306 - val_loss: 0.6739 - val_accuracy: 0.8073\n",
            "Epoch 196/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5802 - accuracy: 0.8325 - val_loss: 0.6689 - val_accuracy: 0.8088\n",
            "Epoch 197/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5803 - accuracy: 0.8341 - val_loss: 0.6686 - val_accuracy: 0.8073\n",
            "Epoch 198/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5737 - accuracy: 0.8341 - val_loss: 0.6696 - val_accuracy: 0.8077\n",
            "Epoch 199/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5812 - accuracy: 0.8344 - val_loss: 0.6728 - val_accuracy: 0.8069\n",
            "Epoch 200/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.5826 - accuracy: 0.8329 - val_loss: 0.6684 - val_accuracy: 0.8099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txxNVY-xZzSf",
        "outputId": "5fcfb7cd-49d3-450b-d504-21575f95b265"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 0.7439 - accuracy: 0.7906\n",
            "Test loss: 0.7439432144165039\n",
            "Test accuracy: 0.7906038761138916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28PgCWWU1Iv"
      },
      "source": [
        "### Model 5\n",
        "\n",
        "Comparing performance according to <font color = red>`# of hidden layers`</font> with *Model 2*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3` changed from <font color=red>`2`</font>\n",
        "\n",
        "`# of neurons: 1024, 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1wFohMZayls",
        "outputId": "3e9c0fbb-c76f-44fc-8bab-8ae63c0dcd9e"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(1024, activation='relu', name='hidden_layer1')) # new layer\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,738,506\n",
            "Trainable params: 3,738,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYjDaqRaylt",
        "outputId": "a54dbc37-01f3-438a-a2ce-31850ff39c1f"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 3s 12ms/step - loss: 2.2439 - accuracy: 0.1818 - val_loss: 2.1676 - val_accuracy: 0.2608\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.1387 - accuracy: 0.2482 - val_loss: 2.0553 - val_accuracy: 0.3267\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.0149 - accuracy: 0.3324 - val_loss: 1.9158 - val_accuracy: 0.3894\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.8685 - accuracy: 0.3984 - val_loss: 1.7858 - val_accuracy: 0.4240\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.7426 - accuracy: 0.4562 - val_loss: 1.6652 - val_accuracy: 0.4964\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.6258 - accuracy: 0.5035 - val_loss: 1.5696 - val_accuracy: 0.5254\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.5341 - accuracy: 0.5371 - val_loss: 1.4954 - val_accuracy: 0.5621\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4530 - accuracy: 0.5738 - val_loss: 1.4276 - val_accuracy: 0.5757\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3858 - accuracy: 0.5959 - val_loss: 1.3729 - val_accuracy: 0.5907\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3345 - accuracy: 0.6134 - val_loss: 1.3269 - val_accuracy: 0.6195\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2979 - accuracy: 0.6225 - val_loss: 1.2853 - val_accuracy: 0.6288\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2533 - accuracy: 0.6348 - val_loss: 1.2547 - val_accuracy: 0.6377\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2151 - accuracy: 0.6460 - val_loss: 1.2279 - val_accuracy: 0.6434\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1851 - accuracy: 0.6561 - val_loss: 1.1936 - val_accuracy: 0.6588\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1690 - accuracy: 0.6640 - val_loss: 1.1750 - val_accuracy: 0.6619\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1332 - accuracy: 0.6708 - val_loss: 1.1539 - val_accuracy: 0.6645\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1109 - accuracy: 0.6791 - val_loss: 1.1299 - val_accuracy: 0.6723\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0950 - accuracy: 0.6835 - val_loss: 1.1190 - val_accuracy: 0.6718\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0772 - accuracy: 0.6847 - val_loss: 1.0982 - val_accuracy: 0.6816\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0565 - accuracy: 0.6939 - val_loss: 1.0814 - val_accuracy: 0.6871\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0427 - accuracy: 0.6963 - val_loss: 1.0654 - val_accuracy: 0.6891\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0218 - accuracy: 0.7041 - val_loss: 1.0542 - val_accuracy: 0.6912\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0054 - accuracy: 0.7057 - val_loss: 1.0447 - val_accuracy: 0.6962\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9978 - accuracy: 0.7095 - val_loss: 1.0335 - val_accuracy: 0.6992\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9885 - accuracy: 0.7126 - val_loss: 1.0131 - val_accuracy: 0.7058\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9690 - accuracy: 0.7192 - val_loss: 1.0053 - val_accuracy: 0.7056\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9691 - accuracy: 0.7148 - val_loss: 0.9939 - val_accuracy: 0.7088\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9551 - accuracy: 0.7205 - val_loss: 0.9806 - val_accuracy: 0.7133\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9364 - accuracy: 0.7274 - val_loss: 0.9740 - val_accuracy: 0.7129\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9311 - accuracy: 0.7264 - val_loss: 0.9706 - val_accuracy: 0.7148\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9179 - accuracy: 0.7317 - val_loss: 0.9618 - val_accuracy: 0.7147\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9092 - accuracy: 0.7350 - val_loss: 0.9505 - val_accuracy: 0.7199\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9085 - accuracy: 0.7335 - val_loss: 0.9419 - val_accuracy: 0.7215\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8978 - accuracy: 0.7368 - val_loss: 0.9383 - val_accuracy: 0.7211\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8857 - accuracy: 0.7396 - val_loss: 0.9253 - val_accuracy: 0.7239\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8778 - accuracy: 0.7439 - val_loss: 0.9179 - val_accuracy: 0.7288\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8614 - accuracy: 0.7463 - val_loss: 0.9110 - val_accuracy: 0.7299\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8576 - accuracy: 0.7503 - val_loss: 0.9061 - val_accuracy: 0.7333\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8474 - accuracy: 0.7540 - val_loss: 0.8946 - val_accuracy: 0.7346\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8472 - accuracy: 0.7514 - val_loss: 0.9003 - val_accuracy: 0.7290\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8398 - accuracy: 0.7537 - val_loss: 0.8806 - val_accuracy: 0.7415\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8333 - accuracy: 0.7557 - val_loss: 0.8858 - val_accuracy: 0.7330\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8259 - accuracy: 0.7559 - val_loss: 0.8806 - val_accuracy: 0.7338\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8198 - accuracy: 0.7606 - val_loss: 0.8664 - val_accuracy: 0.7422\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8062 - accuracy: 0.7617 - val_loss: 0.8627 - val_accuracy: 0.7436\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8144 - accuracy: 0.7610 - val_loss: 0.8613 - val_accuracy: 0.7424\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7975 - accuracy: 0.7674 - val_loss: 0.8545 - val_accuracy: 0.7422\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7935 - accuracy: 0.7666 - val_loss: 0.8506 - val_accuracy: 0.7460\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7834 - accuracy: 0.7684 - val_loss: 0.8392 - val_accuracy: 0.7508\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7841 - accuracy: 0.7716 - val_loss: 0.8368 - val_accuracy: 0.7511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29Ed-Fqtaylt",
        "outputId": "acf0bf72-44fd-4ecb-885b-95f2967389de"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 3s 3ms/step - loss: 0.9222 - accuracy: 0.7273\n",
            "Test loss: 0.9221834540367126\n",
            "Test accuracy: 0.7272971868515015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNuIQ-4yU1ge"
      },
      "source": [
        "### Model 6\n",
        "\n",
        "Comparing performance according to <font color=red>`# of hidden layers`</font> with *Model 2* and *Model 5*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 1` changed from <font color=red>`2 and 3`</font>\n",
        "\n",
        "`# of neurons: 512`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qIz9Cdpb5vN",
        "outputId": "605cbbdc-1a6c-42fb-ec9d-9771ec917347"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1')) # the only hidden layer\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_5 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,578,506\n",
            "Trainable params: 1,578,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghKbvpIwb5vO",
        "outputId": "44a293e0-6678-4f0e-ed21-173fd82336d7"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2769 - accuracy: 0.1744 - val_loss: 2.2170 - val_accuracy: 0.1987\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2124 - accuracy: 0.2030 - val_loss: 2.1867 - val_accuracy: 0.2083\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1805 - accuracy: 0.2191 - val_loss: 2.1597 - val_accuracy: 0.2110\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1460 - accuracy: 0.2398 - val_loss: 2.1246 - val_accuracy: 0.2303\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1168 - accuracy: 0.2584 - val_loss: 2.0902 - val_accuracy: 0.2766\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0781 - accuracy: 0.2827 - val_loss: 2.0555 - val_accuracy: 0.3093\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0333 - accuracy: 0.3131 - val_loss: 2.0147 - val_accuracy: 0.3372\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9976 - accuracy: 0.3374 - val_loss: 1.9758 - val_accuracy: 0.3565\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9579 - accuracy: 0.3599 - val_loss: 1.9431 - val_accuracy: 0.3541\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9250 - accuracy: 0.3859 - val_loss: 1.9049 - val_accuracy: 0.4160\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8815 - accuracy: 0.4120 - val_loss: 1.8661 - val_accuracy: 0.4190\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8452 - accuracy: 0.4285 - val_loss: 1.8322 - val_accuracy: 0.4352\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8108 - accuracy: 0.4487 - val_loss: 1.7971 - val_accuracy: 0.4578\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7762 - accuracy: 0.4672 - val_loss: 1.7652 - val_accuracy: 0.4797\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7457 - accuracy: 0.4789 - val_loss: 1.7299 - val_accuracy: 0.4979\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7118 - accuracy: 0.4996 - val_loss: 1.7017 - val_accuracy: 0.5097\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6798 - accuracy: 0.5110 - val_loss: 1.6756 - val_accuracy: 0.4984\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6487 - accuracy: 0.5226 - val_loss: 1.6449 - val_accuracy: 0.5226\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6247 - accuracy: 0.5339 - val_loss: 1.6192 - val_accuracy: 0.5426\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5946 - accuracy: 0.5459 - val_loss: 1.5921 - val_accuracy: 0.5559\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5636 - accuracy: 0.5589 - val_loss: 1.5712 - val_accuracy: 0.5579\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5473 - accuracy: 0.5665 - val_loss: 1.5476 - val_accuracy: 0.5627\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5161 - accuracy: 0.5783 - val_loss: 1.5245 - val_accuracy: 0.5753\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4983 - accuracy: 0.5852 - val_loss: 1.5053 - val_accuracy: 0.5739\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4783 - accuracy: 0.5898 - val_loss: 1.4821 - val_accuracy: 0.5879\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4548 - accuracy: 0.5973 - val_loss: 1.4640 - val_accuracy: 0.6018\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4412 - accuracy: 0.6031 - val_loss: 1.4498 - val_accuracy: 0.6072\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4212 - accuracy: 0.6080 - val_loss: 1.4289 - val_accuracy: 0.6120\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4098 - accuracy: 0.6113 - val_loss: 1.4144 - val_accuracy: 0.6029\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3839 - accuracy: 0.6165 - val_loss: 1.3989 - val_accuracy: 0.6072\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3712 - accuracy: 0.6241 - val_loss: 1.3833 - val_accuracy: 0.6266\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3514 - accuracy: 0.6304 - val_loss: 1.3681 - val_accuracy: 0.6354\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3388 - accuracy: 0.6314 - val_loss: 1.3556 - val_accuracy: 0.6310\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3200 - accuracy: 0.6439 - val_loss: 1.3404 - val_accuracy: 0.6313\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3125 - accuracy: 0.6397 - val_loss: 1.3269 - val_accuracy: 0.6350\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2953 - accuracy: 0.6467 - val_loss: 1.3150 - val_accuracy: 0.6381\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2857 - accuracy: 0.6477 - val_loss: 1.3022 - val_accuracy: 0.6462\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2709 - accuracy: 0.6558 - val_loss: 1.2909 - val_accuracy: 0.6441\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2560 - accuracy: 0.6577 - val_loss: 1.2802 - val_accuracy: 0.6518\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2482 - accuracy: 0.6588 - val_loss: 1.2725 - val_accuracy: 0.6604\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2420 - accuracy: 0.6615 - val_loss: 1.2609 - val_accuracy: 0.6570\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2260 - accuracy: 0.6674 - val_loss: 1.2486 - val_accuracy: 0.6604\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2204 - accuracy: 0.6673 - val_loss: 1.2381 - val_accuracy: 0.6615\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.2096 - accuracy: 0.6730 - val_loss: 1.2309 - val_accuracy: 0.6577\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1968 - accuracy: 0.6740 - val_loss: 1.2192 - val_accuracy: 0.6677\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1861 - accuracy: 0.6754 - val_loss: 1.2112 - val_accuracy: 0.6701\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1788 - accuracy: 0.6793 - val_loss: 1.2014 - val_accuracy: 0.6721\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1618 - accuracy: 0.6854 - val_loss: 1.1942 - val_accuracy: 0.6747\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1576 - accuracy: 0.6853 - val_loss: 1.1879 - val_accuracy: 0.6734\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.1584 - accuracy: 0.6843 - val_loss: 1.1801 - val_accuracy: 0.6740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIWw7tkpb5vO",
        "outputId": "033d3d79-4c98-441d-d8fc-74dc52493b22"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 1.2333 - accuracy: 0.6485\n",
            "Test loss: 1.233284831047058\n",
            "Test accuracy: 0.6484711170196533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQt84o7rU4bQ"
      },
      "source": [
        "### Model 7\n",
        "\n",
        "Comparing performance according to <font color=red>`# of neurons`</font> with *Model 5*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 256, 64, 16` changed from <font color=red>`1024, 512, 128`</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amb3S_5lcxpY",
        "outputId": "4b0ed1aa-5a13-45c6-fecd-091db3979b9e"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(256, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(64, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(16, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_6 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 256)               786688    \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 16)                1040      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                170       \n",
            "=================================================================\n",
            "Total params: 804,346\n",
            "Trainable params: 804,346\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVZP6QpEcxpj",
        "outputId": "4c4fcb16-27a0-4c62-dcb7-6a90b532a975"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2890 - accuracy: 0.1045 - val_loss: 2.2679 - val_accuracy: 0.1849\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2620 - accuracy: 0.1920 - val_loss: 2.2514 - val_accuracy: 0.2068\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2400 - accuracy: 0.2113 - val_loss: 2.2259 - val_accuracy: 0.2329\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.2180 - accuracy: 0.2370 - val_loss: 2.2001 - val_accuracy: 0.2637\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1918 - accuracy: 0.2708 - val_loss: 2.1747 - val_accuracy: 0.2838\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1631 - accuracy: 0.2897 - val_loss: 2.1475 - val_accuracy: 0.2999\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1348 - accuracy: 0.3073 - val_loss: 2.1225 - val_accuracy: 0.3164\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.1087 - accuracy: 0.3212 - val_loss: 2.0938 - val_accuracy: 0.3245\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0794 - accuracy: 0.3344 - val_loss: 2.0673 - val_accuracy: 0.3435\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0477 - accuracy: 0.3527 - val_loss: 2.0354 - val_accuracy: 0.3569\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 2.0169 - accuracy: 0.3678 - val_loss: 2.0058 - val_accuracy: 0.3758\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9900 - accuracy: 0.3796 - val_loss: 1.9799 - val_accuracy: 0.3889\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9562 - accuracy: 0.3902 - val_loss: 1.9516 - val_accuracy: 0.3928\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9333 - accuracy: 0.3953 - val_loss: 1.9255 - val_accuracy: 0.3940\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.9046 - accuracy: 0.4037 - val_loss: 1.9015 - val_accuracy: 0.4021\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8851 - accuracy: 0.4041 - val_loss: 1.8764 - val_accuracy: 0.4042\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8535 - accuracy: 0.4155 - val_loss: 1.8541 - val_accuracy: 0.4102\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8323 - accuracy: 0.4208 - val_loss: 1.8308 - val_accuracy: 0.4202\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.8074 - accuracy: 0.4307 - val_loss: 1.8083 - val_accuracy: 0.4240\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7935 - accuracy: 0.4321 - val_loss: 1.7901 - val_accuracy: 0.4263\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7676 - accuracy: 0.4383 - val_loss: 1.7702 - val_accuracy: 0.4309\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7419 - accuracy: 0.4479 - val_loss: 1.7527 - val_accuracy: 0.4376\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7275 - accuracy: 0.4500 - val_loss: 1.7324 - val_accuracy: 0.4473\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.7027 - accuracy: 0.4616 - val_loss: 1.7143 - val_accuracy: 0.4535\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6922 - accuracy: 0.4634 - val_loss: 1.6985 - val_accuracy: 0.4546\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6735 - accuracy: 0.4695 - val_loss: 1.6817 - val_accuracy: 0.4621\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6525 - accuracy: 0.4747 - val_loss: 1.6654 - val_accuracy: 0.4715\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6354 - accuracy: 0.4807 - val_loss: 1.6517 - val_accuracy: 0.4805\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6202 - accuracy: 0.4888 - val_loss: 1.6331 - val_accuracy: 0.4791\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.6081 - accuracy: 0.4915 - val_loss: 1.6211 - val_accuracy: 0.4900\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5926 - accuracy: 0.4972 - val_loss: 1.6050 - val_accuracy: 0.4890\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5773 - accuracy: 0.5051 - val_loss: 1.5900 - val_accuracy: 0.4949\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5569 - accuracy: 0.5111 - val_loss: 1.5791 - val_accuracy: 0.5035\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5447 - accuracy: 0.5140 - val_loss: 1.5651 - val_accuracy: 0.5033\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5320 - accuracy: 0.5167 - val_loss: 1.5518 - val_accuracy: 0.5128\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5250 - accuracy: 0.5207 - val_loss: 1.5390 - val_accuracy: 0.5116\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.5126 - accuracy: 0.5208 - val_loss: 1.5293 - val_accuracy: 0.5207\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4944 - accuracy: 0.5303 - val_loss: 1.5150 - val_accuracy: 0.5243\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4890 - accuracy: 0.5306 - val_loss: 1.5047 - val_accuracy: 0.5272\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4766 - accuracy: 0.5381 - val_loss: 1.4963 - val_accuracy: 0.5347\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4643 - accuracy: 0.5383 - val_loss: 1.4862 - val_accuracy: 0.5370\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4539 - accuracy: 0.5442 - val_loss: 1.4745 - val_accuracy: 0.5371\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4420 - accuracy: 0.5486 - val_loss: 1.4645 - val_accuracy: 0.5445\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4315 - accuracy: 0.5530 - val_loss: 1.4532 - val_accuracy: 0.5448\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4224 - accuracy: 0.5542 - val_loss: 1.4433 - val_accuracy: 0.5504\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.4118 - accuracy: 0.5579 - val_loss: 1.4349 - val_accuracy: 0.5532\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3982 - accuracy: 0.5640 - val_loss: 1.4227 - val_accuracy: 0.5632\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3928 - accuracy: 0.5657 - val_loss: 1.4178 - val_accuracy: 0.5646\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3828 - accuracy: 0.5705 - val_loss: 1.4048 - val_accuracy: 0.5646\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 7ms/step - loss: 1.3711 - accuracy: 0.5773 - val_loss: 1.3972 - val_accuracy: 0.5690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A12jfr8hcxpk",
        "outputId": "f4684996-a030-42f3-b7e5-d6d6ada1f2f1"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 2s 3ms/step - loss: 1.4396 - accuracy: 0.5524\n",
            "Test loss: 1.4396333694458008\n",
            "Test accuracy: 0.5524354577064514\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdMRT6RMU5oq"
      },
      "source": [
        "### Model 8\n",
        "\n",
        "Comparing performance according to <font color=red>`# of neurons`</font> with *Model 5 and Model 7*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 512, 128, 64` changed from <font color=red>`1024, 512, 128 and 256, 64, 16`</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl_Q54aAdIqr",
        "outputId": "e662dfa4-6952-40a1-c520-b81992edf664"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(64, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_7 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 512)               1573376   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 1,647,946\n",
            "Trainable params: 1,647,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNBxga1qdIrA",
        "outputId": "2e006bec-e6c8-4112-beee-adb3c4c7e330"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 3s 9ms/step - loss: 2.2529 - accuracy: 0.1776 - val_loss: 2.2151 - val_accuracy: 0.2039\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.2099 - accuracy: 0.2023 - val_loss: 2.1858 - val_accuracy: 0.2230\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.1710 - accuracy: 0.2289 - val_loss: 2.1440 - val_accuracy: 0.2388\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.1282 - accuracy: 0.2541 - val_loss: 2.0967 - val_accuracy: 0.2738\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.0785 - accuracy: 0.2866 - val_loss: 2.0413 - val_accuracy: 0.3329\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 2.0218 - accuracy: 0.3225 - val_loss: 1.9810 - val_accuracy: 0.3507\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.9499 - accuracy: 0.3639 - val_loss: 1.9126 - val_accuracy: 0.3711\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.8844 - accuracy: 0.3947 - val_loss: 1.8484 - val_accuracy: 0.4021\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.8184 - accuracy: 0.4238 - val_loss: 1.7913 - val_accuracy: 0.4448\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.7573 - accuracy: 0.4521 - val_loss: 1.7274 - val_accuracy: 0.4689\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.6959 - accuracy: 0.4806 - val_loss: 1.6637 - val_accuracy: 0.4984\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.6388 - accuracy: 0.5037 - val_loss: 1.6129 - val_accuracy: 0.5042\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.5747 - accuracy: 0.5295 - val_loss: 1.5648 - val_accuracy: 0.5470\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.5257 - accuracy: 0.5507 - val_loss: 1.5221 - val_accuracy: 0.5428\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.4801 - accuracy: 0.5646 - val_loss: 1.4824 - val_accuracy: 0.5637\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.4516 - accuracy: 0.5769 - val_loss: 1.4468 - val_accuracy: 0.5856\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.4104 - accuracy: 0.5912 - val_loss: 1.4125 - val_accuracy: 0.5977\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3748 - accuracy: 0.6021 - val_loss: 1.3829 - val_accuracy: 0.6055\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3417 - accuracy: 0.6148 - val_loss: 1.3556 - val_accuracy: 0.6132\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3240 - accuracy: 0.6171 - val_loss: 1.3308 - val_accuracy: 0.6233\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2971 - accuracy: 0.6276 - val_loss: 1.3093 - val_accuracy: 0.6275\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2728 - accuracy: 0.6343 - val_loss: 1.2860 - val_accuracy: 0.6349\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2495 - accuracy: 0.6405 - val_loss: 1.2693 - val_accuracy: 0.6395\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2351 - accuracy: 0.6444 - val_loss: 1.2539 - val_accuracy: 0.6392\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2129 - accuracy: 0.6514 - val_loss: 1.2369 - val_accuracy: 0.6415\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.2023 - accuracy: 0.6552 - val_loss: 1.2199 - val_accuracy: 0.6520\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1820 - accuracy: 0.6603 - val_loss: 1.2028 - val_accuracy: 0.6616\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1671 - accuracy: 0.6653 - val_loss: 1.1912 - val_accuracy: 0.6596\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1528 - accuracy: 0.6671 - val_loss: 1.1783 - val_accuracy: 0.6643\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1432 - accuracy: 0.6734 - val_loss: 1.1654 - val_accuracy: 0.6658\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1253 - accuracy: 0.6753 - val_loss: 1.1580 - val_accuracy: 0.6679\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1189 - accuracy: 0.6796 - val_loss: 1.1469 - val_accuracy: 0.6717\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1048 - accuracy: 0.6836 - val_loss: 1.1355 - val_accuracy: 0.6736\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0985 - accuracy: 0.6858 - val_loss: 1.1214 - val_accuracy: 0.6798\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0785 - accuracy: 0.6921 - val_loss: 1.1181 - val_accuracy: 0.6781\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0769 - accuracy: 0.6900 - val_loss: 1.1084 - val_accuracy: 0.6817\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0656 - accuracy: 0.6936 - val_loss: 1.0978 - val_accuracy: 0.6837\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0574 - accuracy: 0.6970 - val_loss: 1.0913 - val_accuracy: 0.6835\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0494 - accuracy: 0.6962 - val_loss: 1.0831 - val_accuracy: 0.6891\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0396 - accuracy: 0.6996 - val_loss: 1.0755 - val_accuracy: 0.6901\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0307 - accuracy: 0.7051 - val_loss: 1.0695 - val_accuracy: 0.6890\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0238 - accuracy: 0.7045 - val_loss: 1.0617 - val_accuracy: 0.6934\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0209 - accuracy: 0.7087 - val_loss: 1.0540 - val_accuracy: 0.6931\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0141 - accuracy: 0.7074 - val_loss: 1.0484 - val_accuracy: 0.6981\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0103 - accuracy: 0.7073 - val_loss: 1.0428 - val_accuracy: 0.6966\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9960 - accuracy: 0.7129 - val_loss: 1.0337 - val_accuracy: 0.7014\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9993 - accuracy: 0.7108 - val_loss: 1.0349 - val_accuracy: 0.7003\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9832 - accuracy: 0.7149 - val_loss: 1.0244 - val_accuracy: 0.7015\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9810 - accuracy: 0.7170 - val_loss: 1.0202 - val_accuracy: 0.7068\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.9777 - accuracy: 0.7167 - val_loss: 1.0166 - val_accuracy: 0.7039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c95F2M1dIrA",
        "outputId": "29f6d4f4-cc00-487d-d1cb-ef44c33b15c6"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 3s 3ms/step - loss: 1.0963 - accuracy: 0.6773\n",
            "Test loss: 1.0963495969772339\n",
            "Test accuracy: 0.6773202419281006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc6_k1qW8bvF"
      },
      "source": [
        "###Model 9\n",
        "\n",
        "Trying to guess best performing hyperparameters from previous comparions without paying attention to time and GPU power/RAM used.\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 200`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 128`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbWyMmYI9AKL",
        "outputId": "b35d7475-654d-4a3d-855f-79dd28ea2d46"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(1024, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_8 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,738,506\n",
            "Trainable params: 3,738,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhInXyDr9AKM",
        "outputId": "2e1e8540-04c2-453a-b9e8-7cbd23711a17"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=200, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "229/229 [==============================] - 3s 11ms/step - loss: 2.2419 - accuracy: 0.1852 - val_loss: 2.1740 - val_accuracy: 0.2469\n",
            "Epoch 2/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.1474 - accuracy: 0.2511 - val_loss: 2.0694 - val_accuracy: 0.3003\n",
            "Epoch 3/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.0315 - accuracy: 0.3150 - val_loss: 1.9555 - val_accuracy: 0.3320\n",
            "Epoch 4/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.9162 - accuracy: 0.3745 - val_loss: 1.8285 - val_accuracy: 0.4132\n",
            "Epoch 5/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.7852 - accuracy: 0.4431 - val_loss: 1.7170 - val_accuracy: 0.4799\n",
            "Epoch 6/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.6691 - accuracy: 0.4924 - val_loss: 1.6121 - val_accuracy: 0.5337\n",
            "Epoch 7/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.5763 - accuracy: 0.5357 - val_loss: 1.5298 - val_accuracy: 0.5620\n",
            "Epoch 8/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4916 - accuracy: 0.5675 - val_loss: 1.4592 - val_accuracy: 0.5758\n",
            "Epoch 9/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4197 - accuracy: 0.5898 - val_loss: 1.3966 - val_accuracy: 0.6014\n",
            "Epoch 10/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3617 - accuracy: 0.6102 - val_loss: 1.3496 - val_accuracy: 0.6119\n",
            "Epoch 11/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.3154 - accuracy: 0.6226 - val_loss: 1.3052 - val_accuracy: 0.6295\n",
            "Epoch 12/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2630 - accuracy: 0.6390 - val_loss: 1.2666 - val_accuracy: 0.6374\n",
            "Epoch 13/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2271 - accuracy: 0.6490 - val_loss: 1.2294 - val_accuracy: 0.6446\n",
            "Epoch 14/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1882 - accuracy: 0.6583 - val_loss: 1.2004 - val_accuracy: 0.6560\n",
            "Epoch 15/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1630 - accuracy: 0.6703 - val_loss: 1.1739 - val_accuracy: 0.6698\n",
            "Epoch 16/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1397 - accuracy: 0.6724 - val_loss: 1.1568 - val_accuracy: 0.6721\n",
            "Epoch 17/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1181 - accuracy: 0.6815 - val_loss: 1.1299 - val_accuracy: 0.6802\n",
            "Epoch 18/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0930 - accuracy: 0.6859 - val_loss: 1.1149 - val_accuracy: 0.6738\n",
            "Epoch 19/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0734 - accuracy: 0.6935 - val_loss: 1.1016 - val_accuracy: 0.6777\n",
            "Epoch 20/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0540 - accuracy: 0.6961 - val_loss: 1.0917 - val_accuracy: 0.6811\n",
            "Epoch 21/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0426 - accuracy: 0.7012 - val_loss: 1.0638 - val_accuracy: 0.6944\n",
            "Epoch 22/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0220 - accuracy: 0.7083 - val_loss: 1.0671 - val_accuracy: 0.6868\n",
            "Epoch 23/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0038 - accuracy: 0.7113 - val_loss: 1.0383 - val_accuracy: 0.6983\n",
            "Epoch 24/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0000 - accuracy: 0.7121 - val_loss: 1.0324 - val_accuracy: 0.7029\n",
            "Epoch 25/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9862 - accuracy: 0.7160 - val_loss: 1.0145 - val_accuracy: 0.7090\n",
            "Epoch 26/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9801 - accuracy: 0.7177 - val_loss: 1.0026 - val_accuracy: 0.7103\n",
            "Epoch 27/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9609 - accuracy: 0.7211 - val_loss: 0.9918 - val_accuracy: 0.7161\n",
            "Epoch 28/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9471 - accuracy: 0.7259 - val_loss: 0.9813 - val_accuracy: 0.7140\n",
            "Epoch 29/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9404 - accuracy: 0.7274 - val_loss: 0.9732 - val_accuracy: 0.7168\n",
            "Epoch 30/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9289 - accuracy: 0.7301 - val_loss: 0.9652 - val_accuracy: 0.7189\n",
            "Epoch 31/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9206 - accuracy: 0.7314 - val_loss: 0.9600 - val_accuracy: 0.7185\n",
            "Epoch 32/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9149 - accuracy: 0.7362 - val_loss: 0.9493 - val_accuracy: 0.7224\n",
            "Epoch 33/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9004 - accuracy: 0.7379 - val_loss: 0.9451 - val_accuracy: 0.7213\n",
            "Epoch 34/200\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 0.8967 - accuracy: 0.7385 - val_loss: 0.9350 - val_accuracy: 0.7247\n",
            "Epoch 35/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8821 - accuracy: 0.7426 - val_loss: 0.9294 - val_accuracy: 0.7253\n",
            "Epoch 36/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8855 - accuracy: 0.7417 - val_loss: 0.9221 - val_accuracy: 0.7295\n",
            "Epoch 37/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8701 - accuracy: 0.7448 - val_loss: 0.9187 - val_accuracy: 0.7291\n",
            "Epoch 38/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8593 - accuracy: 0.7498 - val_loss: 0.9167 - val_accuracy: 0.7298\n",
            "Epoch 39/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8586 - accuracy: 0.7488 - val_loss: 0.9020 - val_accuracy: 0.7342\n",
            "Epoch 40/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8468 - accuracy: 0.7510 - val_loss: 0.8987 - val_accuracy: 0.7353\n",
            "Epoch 41/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8434 - accuracy: 0.7534 - val_loss: 0.8904 - val_accuracy: 0.7363\n",
            "Epoch 42/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8279 - accuracy: 0.7562 - val_loss: 0.8791 - val_accuracy: 0.7397\n",
            "Epoch 43/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8281 - accuracy: 0.7566 - val_loss: 0.8818 - val_accuracy: 0.7355\n",
            "Epoch 44/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8208 - accuracy: 0.7602 - val_loss: 0.8721 - val_accuracy: 0.7407\n",
            "Epoch 45/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8099 - accuracy: 0.7642 - val_loss: 0.8643 - val_accuracy: 0.7420\n",
            "Epoch 46/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8057 - accuracy: 0.7625 - val_loss: 0.8596 - val_accuracy: 0.7422\n",
            "Epoch 47/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8044 - accuracy: 0.7666 - val_loss: 0.8532 - val_accuracy: 0.7463\n",
            "Epoch 48/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7932 - accuracy: 0.7686 - val_loss: 0.8471 - val_accuracy: 0.7482\n",
            "Epoch 49/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7874 - accuracy: 0.7687 - val_loss: 0.8403 - val_accuracy: 0.7510\n",
            "Epoch 50/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7854 - accuracy: 0.7700 - val_loss: 0.8358 - val_accuracy: 0.7508\n",
            "Epoch 51/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7790 - accuracy: 0.7706 - val_loss: 0.8303 - val_accuracy: 0.7528\n",
            "Epoch 52/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7733 - accuracy: 0.7722 - val_loss: 0.8301 - val_accuracy: 0.7533\n",
            "Epoch 53/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7671 - accuracy: 0.7770 - val_loss: 0.8269 - val_accuracy: 0.7565\n",
            "Epoch 54/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7577 - accuracy: 0.7789 - val_loss: 0.8247 - val_accuracy: 0.7566\n",
            "Epoch 55/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7575 - accuracy: 0.7779 - val_loss: 0.8194 - val_accuracy: 0.7589\n",
            "Epoch 56/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7484 - accuracy: 0.7800 - val_loss: 0.8095 - val_accuracy: 0.7601\n",
            "Epoch 57/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7443 - accuracy: 0.7839 - val_loss: 0.8114 - val_accuracy: 0.7559\n",
            "Epoch 58/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7402 - accuracy: 0.7856 - val_loss: 0.7974 - val_accuracy: 0.7637\n",
            "Epoch 59/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7346 - accuracy: 0.7839 - val_loss: 0.7952 - val_accuracy: 0.7627\n",
            "Epoch 60/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7269 - accuracy: 0.7870 - val_loss: 0.7910 - val_accuracy: 0.7622\n",
            "Epoch 61/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7349 - accuracy: 0.7850 - val_loss: 0.7869 - val_accuracy: 0.7666\n",
            "Epoch 62/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7209 - accuracy: 0.7899 - val_loss: 0.7821 - val_accuracy: 0.7673\n",
            "Epoch 63/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7086 - accuracy: 0.7919 - val_loss: 0.7815 - val_accuracy: 0.7661\n",
            "Epoch 64/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7051 - accuracy: 0.7930 - val_loss: 0.7743 - val_accuracy: 0.7707\n",
            "Epoch 65/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7074 - accuracy: 0.7942 - val_loss: 0.7693 - val_accuracy: 0.7722\n",
            "Epoch 66/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7070 - accuracy: 0.7941 - val_loss: 0.7704 - val_accuracy: 0.7731\n",
            "Epoch 67/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6989 - accuracy: 0.7951 - val_loss: 0.7644 - val_accuracy: 0.7720\n",
            "Epoch 68/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7033 - accuracy: 0.7951 - val_loss: 0.7599 - val_accuracy: 0.7746\n",
            "Epoch 69/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6935 - accuracy: 0.7974 - val_loss: 0.7558 - val_accuracy: 0.7731\n",
            "Epoch 70/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6857 - accuracy: 0.7979 - val_loss: 0.7732 - val_accuracy: 0.7648\n",
            "Epoch 71/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6848 - accuracy: 0.7957 - val_loss: 0.7616 - val_accuracy: 0.7722\n",
            "Epoch 72/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6873 - accuracy: 0.7985 - val_loss: 0.7434 - val_accuracy: 0.7808\n",
            "Epoch 73/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6713 - accuracy: 0.8039 - val_loss: 0.7462 - val_accuracy: 0.7808\n",
            "Epoch 74/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6691 - accuracy: 0.8039 - val_loss: 0.7383 - val_accuracy: 0.7830\n",
            "Epoch 75/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6675 - accuracy: 0.8034 - val_loss: 0.7443 - val_accuracy: 0.7785\n",
            "Epoch 76/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6610 - accuracy: 0.8074 - val_loss: 0.7484 - val_accuracy: 0.7737\n",
            "Epoch 77/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6570 - accuracy: 0.8059 - val_loss: 0.7341 - val_accuracy: 0.7832\n",
            "Epoch 78/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6513 - accuracy: 0.8134 - val_loss: 0.7295 - val_accuracy: 0.7871\n",
            "Epoch 79/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6544 - accuracy: 0.8097 - val_loss: 0.7250 - val_accuracy: 0.7882\n",
            "Epoch 80/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6460 - accuracy: 0.8093 - val_loss: 0.7247 - val_accuracy: 0.7871\n",
            "Epoch 81/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6480 - accuracy: 0.8109 - val_loss: 0.7206 - val_accuracy: 0.7857\n",
            "Epoch 82/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6370 - accuracy: 0.8149 - val_loss: 0.7251 - val_accuracy: 0.7865\n",
            "Epoch 83/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6399 - accuracy: 0.8131 - val_loss: 0.7158 - val_accuracy: 0.7901\n",
            "Epoch 84/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6323 - accuracy: 0.8168 - val_loss: 0.7105 - val_accuracy: 0.7940\n",
            "Epoch 85/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6231 - accuracy: 0.8171 - val_loss: 0.7084 - val_accuracy: 0.7925\n",
            "Epoch 86/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6238 - accuracy: 0.8193 - val_loss: 0.7114 - val_accuracy: 0.7921\n",
            "Epoch 87/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6180 - accuracy: 0.8186 - val_loss: 0.7113 - val_accuracy: 0.7891\n",
            "Epoch 88/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6130 - accuracy: 0.8233 - val_loss: 0.7055 - val_accuracy: 0.7937\n",
            "Epoch 89/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6114 - accuracy: 0.8206 - val_loss: 0.6993 - val_accuracy: 0.7933\n",
            "Epoch 90/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6085 - accuracy: 0.8245 - val_loss: 0.6961 - val_accuracy: 0.7963\n",
            "Epoch 91/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6079 - accuracy: 0.8235 - val_loss: 0.6964 - val_accuracy: 0.7942\n",
            "Epoch 92/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6068 - accuracy: 0.8235 - val_loss: 0.6932 - val_accuracy: 0.7987\n",
            "Epoch 93/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6018 - accuracy: 0.8256 - val_loss: 0.6958 - val_accuracy: 0.7970\n",
            "Epoch 94/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6069 - accuracy: 0.8250 - val_loss: 0.6941 - val_accuracy: 0.7968\n",
            "Epoch 95/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6011 - accuracy: 0.8269 - val_loss: 0.6879 - val_accuracy: 0.7991\n",
            "Epoch 96/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5951 - accuracy: 0.8269 - val_loss: 0.6835 - val_accuracy: 0.8001\n",
            "Epoch 97/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5878 - accuracy: 0.8294 - val_loss: 0.6887 - val_accuracy: 0.7960\n",
            "Epoch 98/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5919 - accuracy: 0.8282 - val_loss: 0.6851 - val_accuracy: 0.8008\n",
            "Epoch 99/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5818 - accuracy: 0.8309 - val_loss: 0.6763 - val_accuracy: 0.8034\n",
            "Epoch 100/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5775 - accuracy: 0.8324 - val_loss: 0.6775 - val_accuracy: 0.8006\n",
            "Epoch 101/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5796 - accuracy: 0.8322 - val_loss: 0.6765 - val_accuracy: 0.8017\n",
            "Epoch 102/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5806 - accuracy: 0.8329 - val_loss: 0.6698 - val_accuracy: 0.8032\n",
            "Epoch 103/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5719 - accuracy: 0.8337 - val_loss: 0.6733 - val_accuracy: 0.8066\n",
            "Epoch 104/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5647 - accuracy: 0.8359 - val_loss: 0.6705 - val_accuracy: 0.8038\n",
            "Epoch 105/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5610 - accuracy: 0.8378 - val_loss: 0.6664 - val_accuracy: 0.8055\n",
            "Epoch 106/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5715 - accuracy: 0.8341 - val_loss: 0.6639 - val_accuracy: 0.8043\n",
            "Epoch 107/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5687 - accuracy: 0.8360 - val_loss: 0.6652 - val_accuracy: 0.8075\n",
            "Epoch 108/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5687 - accuracy: 0.8375 - val_loss: 0.6670 - val_accuracy: 0.8060\n",
            "Epoch 109/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5635 - accuracy: 0.8388 - val_loss: 0.6698 - val_accuracy: 0.8031\n",
            "Epoch 110/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5585 - accuracy: 0.8398 - val_loss: 0.6596 - val_accuracy: 0.8104\n",
            "Epoch 111/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5589 - accuracy: 0.8377 - val_loss: 0.6577 - val_accuracy: 0.8080\n",
            "Epoch 112/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5612 - accuracy: 0.8375 - val_loss: 0.6542 - val_accuracy: 0.8102\n",
            "Epoch 113/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5436 - accuracy: 0.8421 - val_loss: 0.6528 - val_accuracy: 0.8117\n",
            "Epoch 114/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5446 - accuracy: 0.8431 - val_loss: 0.6614 - val_accuracy: 0.8084\n",
            "Epoch 115/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5483 - accuracy: 0.8403 - val_loss: 0.6476 - val_accuracy: 0.8124\n",
            "Epoch 116/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5415 - accuracy: 0.8433 - val_loss: 0.6448 - val_accuracy: 0.8120\n",
            "Epoch 117/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5441 - accuracy: 0.8426 - val_loss: 0.6496 - val_accuracy: 0.8122\n",
            "Epoch 118/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5410 - accuracy: 0.8425 - val_loss: 0.6423 - val_accuracy: 0.8148\n",
            "Epoch 119/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5407 - accuracy: 0.8434 - val_loss: 0.6482 - val_accuracy: 0.8097\n",
            "Epoch 120/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5297 - accuracy: 0.8466 - val_loss: 0.6446 - val_accuracy: 0.8120\n",
            "Epoch 121/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5261 - accuracy: 0.8481 - val_loss: 0.6502 - val_accuracy: 0.8105\n",
            "Epoch 122/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5301 - accuracy: 0.8435 - val_loss: 0.6426 - val_accuracy: 0.8140\n",
            "Epoch 123/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5280 - accuracy: 0.8483 - val_loss: 0.6382 - val_accuracy: 0.8149\n",
            "Epoch 124/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5261 - accuracy: 0.8472 - val_loss: 0.6453 - val_accuracy: 0.8105\n",
            "Epoch 125/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5262 - accuracy: 0.8484 - val_loss: 0.6392 - val_accuracy: 0.8167\n",
            "Epoch 126/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5221 - accuracy: 0.8489 - val_loss: 0.6364 - val_accuracy: 0.8163\n",
            "Epoch 127/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5237 - accuracy: 0.8505 - val_loss: 0.6380 - val_accuracy: 0.8129\n",
            "Epoch 128/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5151 - accuracy: 0.8510 - val_loss: 0.6351 - val_accuracy: 0.8182\n",
            "Epoch 129/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5217 - accuracy: 0.8502 - val_loss: 0.6352 - val_accuracy: 0.8174\n",
            "Epoch 130/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5209 - accuracy: 0.8480 - val_loss: 0.6286 - val_accuracy: 0.8200\n",
            "Epoch 131/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5090 - accuracy: 0.8514 - val_loss: 0.6354 - val_accuracy: 0.8152\n",
            "Epoch 132/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5102 - accuracy: 0.8542 - val_loss: 0.6382 - val_accuracy: 0.8161\n",
            "Epoch 133/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5097 - accuracy: 0.8537 - val_loss: 0.6304 - val_accuracy: 0.8215\n",
            "Epoch 134/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5080 - accuracy: 0.8519 - val_loss: 0.6218 - val_accuracy: 0.8220\n",
            "Epoch 135/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5063 - accuracy: 0.8537 - val_loss: 0.6225 - val_accuracy: 0.8203\n",
            "Epoch 136/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5033 - accuracy: 0.8556 - val_loss: 0.6336 - val_accuracy: 0.8174\n",
            "Epoch 137/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4957 - accuracy: 0.8585 - val_loss: 0.6254 - val_accuracy: 0.8198\n",
            "Epoch 138/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4976 - accuracy: 0.8574 - val_loss: 0.6187 - val_accuracy: 0.8221\n",
            "Epoch 139/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4926 - accuracy: 0.8570 - val_loss: 0.6219 - val_accuracy: 0.8212\n",
            "Epoch 140/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4969 - accuracy: 0.8579 - val_loss: 0.6284 - val_accuracy: 0.8176\n",
            "Epoch 141/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4890 - accuracy: 0.8597 - val_loss: 0.6239 - val_accuracy: 0.8191\n",
            "Epoch 142/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4922 - accuracy: 0.8586 - val_loss: 0.6178 - val_accuracy: 0.8232\n",
            "Epoch 143/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4827 - accuracy: 0.8632 - val_loss: 0.6148 - val_accuracy: 0.8219\n",
            "Epoch 144/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4816 - accuracy: 0.8614 - val_loss: 0.6149 - val_accuracy: 0.8235\n",
            "Epoch 145/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4850 - accuracy: 0.8587 - val_loss: 0.6169 - val_accuracy: 0.8199\n",
            "Epoch 146/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4779 - accuracy: 0.8625 - val_loss: 0.6137 - val_accuracy: 0.8251\n",
            "Epoch 147/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4804 - accuracy: 0.8621 - val_loss: 0.6155 - val_accuracy: 0.8241\n",
            "Epoch 148/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4855 - accuracy: 0.8591 - val_loss: 0.6350 - val_accuracy: 0.8165\n",
            "Epoch 149/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4828 - accuracy: 0.8605 - val_loss: 0.6156 - val_accuracy: 0.8247\n",
            "Epoch 150/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4746 - accuracy: 0.8633 - val_loss: 0.6118 - val_accuracy: 0.8236\n",
            "Epoch 151/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4814 - accuracy: 0.8629 - val_loss: 0.6052 - val_accuracy: 0.8265\n",
            "Epoch 152/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4712 - accuracy: 0.8648 - val_loss: 0.6140 - val_accuracy: 0.8245\n",
            "Epoch 153/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4778 - accuracy: 0.8629 - val_loss: 0.6055 - val_accuracy: 0.8246\n",
            "Epoch 154/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4725 - accuracy: 0.8623 - val_loss: 0.6025 - val_accuracy: 0.8290\n",
            "Epoch 155/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4679 - accuracy: 0.8665 - val_loss: 0.6032 - val_accuracy: 0.8255\n",
            "Epoch 156/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4641 - accuracy: 0.8666 - val_loss: 0.6034 - val_accuracy: 0.8268\n",
            "Epoch 157/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4712 - accuracy: 0.8651 - val_loss: 0.6134 - val_accuracy: 0.8237\n",
            "Epoch 158/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4618 - accuracy: 0.8676 - val_loss: 0.6215 - val_accuracy: 0.8189\n",
            "Epoch 159/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4748 - accuracy: 0.8642 - val_loss: 0.6016 - val_accuracy: 0.8281\n",
            "Epoch 160/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4570 - accuracy: 0.8696 - val_loss: 0.6065 - val_accuracy: 0.8262\n",
            "Epoch 161/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4538 - accuracy: 0.8706 - val_loss: 0.5993 - val_accuracy: 0.8284\n",
            "Epoch 162/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4608 - accuracy: 0.8673 - val_loss: 0.6031 - val_accuracy: 0.8283\n",
            "Epoch 163/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4510 - accuracy: 0.8721 - val_loss: 0.5965 - val_accuracy: 0.8285\n",
            "Epoch 164/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4507 - accuracy: 0.8706 - val_loss: 0.5982 - val_accuracy: 0.8301\n",
            "Epoch 165/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4524 - accuracy: 0.8708 - val_loss: 0.5967 - val_accuracy: 0.8278\n",
            "Epoch 166/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4563 - accuracy: 0.8678 - val_loss: 0.6009 - val_accuracy: 0.8286\n",
            "Epoch 167/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4460 - accuracy: 0.8733 - val_loss: 0.6011 - val_accuracy: 0.8288\n",
            "Epoch 168/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4478 - accuracy: 0.8716 - val_loss: 0.5963 - val_accuracy: 0.8301\n",
            "Epoch 169/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4412 - accuracy: 0.8733 - val_loss: 0.6042 - val_accuracy: 0.8276\n",
            "Epoch 170/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4440 - accuracy: 0.8724 - val_loss: 0.6016 - val_accuracy: 0.8285\n",
            "Epoch 171/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4364 - accuracy: 0.8760 - val_loss: 0.5948 - val_accuracy: 0.8284\n",
            "Epoch 172/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4392 - accuracy: 0.8751 - val_loss: 0.5913 - val_accuracy: 0.8322\n",
            "Epoch 173/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4422 - accuracy: 0.8737 - val_loss: 0.6016 - val_accuracy: 0.8275\n",
            "Epoch 174/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4375 - accuracy: 0.8766 - val_loss: 0.5954 - val_accuracy: 0.8297\n",
            "Epoch 175/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4385 - accuracy: 0.8758 - val_loss: 0.5838 - val_accuracy: 0.8335\n",
            "Epoch 176/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4306 - accuracy: 0.8780 - val_loss: 0.5939 - val_accuracy: 0.8327\n",
            "Epoch 177/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4302 - accuracy: 0.8763 - val_loss: 0.5982 - val_accuracy: 0.8279\n",
            "Epoch 178/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4324 - accuracy: 0.8774 - val_loss: 0.6001 - val_accuracy: 0.8278\n",
            "Epoch 179/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4257 - accuracy: 0.8781 - val_loss: 0.5909 - val_accuracy: 0.8318\n",
            "Epoch 180/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4299 - accuracy: 0.8774 - val_loss: 0.5958 - val_accuracy: 0.8308\n",
            "Epoch 181/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4218 - accuracy: 0.8796 - val_loss: 0.5894 - val_accuracy: 0.8305\n",
            "Epoch 182/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4208 - accuracy: 0.8828 - val_loss: 0.5860 - val_accuracy: 0.8324\n",
            "Epoch 183/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4183 - accuracy: 0.8809 - val_loss: 0.5940 - val_accuracy: 0.8297\n",
            "Epoch 184/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4239 - accuracy: 0.8783 - val_loss: 0.5802 - val_accuracy: 0.8359\n",
            "Epoch 185/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4145 - accuracy: 0.8807 - val_loss: 0.5890 - val_accuracy: 0.8322\n",
            "Epoch 186/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4176 - accuracy: 0.8819 - val_loss: 0.5958 - val_accuracy: 0.8286\n",
            "Epoch 187/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4243 - accuracy: 0.8811 - val_loss: 0.5830 - val_accuracy: 0.8342\n",
            "Epoch 188/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4142 - accuracy: 0.8834 - val_loss: 0.5808 - val_accuracy: 0.8359\n",
            "Epoch 189/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4157 - accuracy: 0.8839 - val_loss: 0.5770 - val_accuracy: 0.8372\n",
            "Epoch 190/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4141 - accuracy: 0.8841 - val_loss: 0.5791 - val_accuracy: 0.8372\n",
            "Epoch 191/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4100 - accuracy: 0.8851 - val_loss: 0.5777 - val_accuracy: 0.8338\n",
            "Epoch 192/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4124 - accuracy: 0.8834 - val_loss: 0.5778 - val_accuracy: 0.8360\n",
            "Epoch 193/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4125 - accuracy: 0.8814 - val_loss: 0.5749 - val_accuracy: 0.8387\n",
            "Epoch 194/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4080 - accuracy: 0.8832 - val_loss: 0.5806 - val_accuracy: 0.8350\n",
            "Epoch 195/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4030 - accuracy: 0.8842 - val_loss: 0.5751 - val_accuracy: 0.8382\n",
            "Epoch 196/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4057 - accuracy: 0.8867 - val_loss: 0.5884 - val_accuracy: 0.8333\n",
            "Epoch 197/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4022 - accuracy: 0.8856 - val_loss: 0.5845 - val_accuracy: 0.8361\n",
            "Epoch 198/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4021 - accuracy: 0.8845 - val_loss: 0.5729 - val_accuracy: 0.8382\n",
            "Epoch 199/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4021 - accuracy: 0.8876 - val_loss: 0.5910 - val_accuracy: 0.8320\n",
            "Epoch 200/200\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3957 - accuracy: 0.8893 - val_loss: 0.5708 - val_accuracy: 0.8386\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYUbPUve9AKM",
        "outputId": "2edb4029-5923-4ef4-ce4d-a3063460cc10"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 3s 3ms/step - loss: 0.6633 - accuracy: 0.8171\n",
            "Test loss: 0.6632614731788635\n",
            "Test accuracy: 0.8170713186264038\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxHylPsSXU-Q"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Dropout** : [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) is a regularization approach. At each training stage, specified percent of individual nodes are dropped out of the network.\n",
        "\n",
        "[Read](https://machinelearningmastery.com/how-to-reduce-overfitting-with-dropout-regularization-in-keras/) MLP Dropout Regularization for further information\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=12lv5VsPFLt6sKEZvgQRyQ7mHeW4l-Vq9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRZpQ4ZmXPVd"
      },
      "source": [
        "#Try adding dropout to the architecture "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k42V-XVpvAAy"
      },
      "source": [
        "### Model 10 (**Dropout**)\n",
        "\n",
        "Comparing performance according to <font color = red>`regularization (dropout)`</font> with *Model 3*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 100`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 128`\n",
        "\n",
        "<font color=red>**WITH DROPOUT**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLb7wsjtu7nC",
        "outputId": "4b62396f-250e-404d-e5aa-8d5d156a8d84"
      },
      "source": [
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(1024, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_9 (Flatten)          (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,738,506\n",
            "Trainable params: 3,738,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAhPZs_Mu7nE",
        "outputId": "39e0dcf6-b557-48e4-aacc-1d4c7cce1dca"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, batch_size=256, epochs=50, validation_split = 0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "229/229 [==============================] - 3s 11ms/step - loss: 2.3187 - accuracy: 0.1524 - val_loss: 2.2351 - val_accuracy: 0.1913\n",
            "Epoch 2/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2485 - accuracy: 0.1850 - val_loss: 2.2137 - val_accuracy: 0.1900\n",
            "Epoch 3/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.2262 - accuracy: 0.1950 - val_loss: 2.1818 - val_accuracy: 0.1959\n",
            "Epoch 4/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.1926 - accuracy: 0.2102 - val_loss: 2.1343 - val_accuracy: 0.2331\n",
            "Epoch 5/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.1529 - accuracy: 0.2278 - val_loss: 2.0718 - val_accuracy: 0.2791\n",
            "Epoch 6/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.0986 - accuracy: 0.2585 - val_loss: 2.0013 - val_accuracy: 0.3324\n",
            "Epoch 7/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.0421 - accuracy: 0.2937 - val_loss: 1.9247 - val_accuracy: 0.3706\n",
            "Epoch 8/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.9772 - accuracy: 0.3285 - val_loss: 1.8508 - val_accuracy: 0.4104\n",
            "Epoch 9/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.9144 - accuracy: 0.3598 - val_loss: 1.7723 - val_accuracy: 0.4323\n",
            "Epoch 10/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.8459 - accuracy: 0.3886 - val_loss: 1.7007 - val_accuracy: 0.4659\n",
            "Epoch 11/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.7817 - accuracy: 0.4156 - val_loss: 1.6280 - val_accuracy: 0.5083\n",
            "Epoch 12/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.7125 - accuracy: 0.4411 - val_loss: 1.5582 - val_accuracy: 0.5261\n",
            "Epoch 13/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.6527 - accuracy: 0.4644 - val_loss: 1.5060 - val_accuracy: 0.5423\n",
            "Epoch 14/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.6005 - accuracy: 0.4837 - val_loss: 1.4530 - val_accuracy: 0.5592\n",
            "Epoch 15/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.5530 - accuracy: 0.5030 - val_loss: 1.4067 - val_accuracy: 0.5799\n",
            "Epoch 16/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.5085 - accuracy: 0.5154 - val_loss: 1.3628 - val_accuracy: 0.6086\n",
            "Epoch 17/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4705 - accuracy: 0.5293 - val_loss: 1.3246 - val_accuracy: 0.6144\n",
            "Epoch 18/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4293 - accuracy: 0.5464 - val_loss: 1.2891 - val_accuracy: 0.6267\n",
            "Epoch 19/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4008 - accuracy: 0.5538 - val_loss: 1.2585 - val_accuracy: 0.6338\n",
            "Epoch 20/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3655 - accuracy: 0.5643 - val_loss: 1.2288 - val_accuracy: 0.6467\n",
            "Epoch 21/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3442 - accuracy: 0.5767 - val_loss: 1.2059 - val_accuracy: 0.6486\n",
            "Epoch 22/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3173 - accuracy: 0.5824 - val_loss: 1.1815 - val_accuracy: 0.6576\n",
            "Epoch 23/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2933 - accuracy: 0.5928 - val_loss: 1.1608 - val_accuracy: 0.6568\n",
            "Epoch 24/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2727 - accuracy: 0.5994 - val_loss: 1.1390 - val_accuracy: 0.6668\n",
            "Epoch 25/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2400 - accuracy: 0.6132 - val_loss: 1.1240 - val_accuracy: 0.6740\n",
            "Epoch 26/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2187 - accuracy: 0.6166 - val_loss: 1.1022 - val_accuracy: 0.6771\n",
            "Epoch 27/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2072 - accuracy: 0.6234 - val_loss: 1.0886 - val_accuracy: 0.6805\n",
            "Epoch 28/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1800 - accuracy: 0.6345 - val_loss: 1.0709 - val_accuracy: 0.6867\n",
            "Epoch 29/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1719 - accuracy: 0.6372 - val_loss: 1.0528 - val_accuracy: 0.6929\n",
            "Epoch 30/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1585 - accuracy: 0.6410 - val_loss: 1.0436 - val_accuracy: 0.6928\n",
            "Epoch 31/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1360 - accuracy: 0.6495 - val_loss: 1.0300 - val_accuracy: 0.6983\n",
            "Epoch 32/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1350 - accuracy: 0.6512 - val_loss: 1.0175 - val_accuracy: 0.7038\n",
            "Epoch 33/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1175 - accuracy: 0.6568 - val_loss: 1.0074 - val_accuracy: 0.7044\n",
            "Epoch 34/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.1035 - accuracy: 0.6603 - val_loss: 0.9954 - val_accuracy: 0.7070\n",
            "Epoch 35/50\n",
            "229/229 [==============================] - 2s 8ms/step - loss: 1.0989 - accuracy: 0.6610 - val_loss: 0.9894 - val_accuracy: 0.7084\n",
            "Epoch 36/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0734 - accuracy: 0.6702 - val_loss: 0.9736 - val_accuracy: 0.7149\n",
            "Epoch 37/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0657 - accuracy: 0.6729 - val_loss: 0.9686 - val_accuracy: 0.7097\n",
            "Epoch 38/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0578 - accuracy: 0.6775 - val_loss: 0.9586 - val_accuracy: 0.7142\n",
            "Epoch 39/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0520 - accuracy: 0.6789 - val_loss: 0.9503 - val_accuracy: 0.7224\n",
            "Epoch 40/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0381 - accuracy: 0.6839 - val_loss: 0.9384 - val_accuracy: 0.7218\n",
            "Epoch 41/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0225 - accuracy: 0.6896 - val_loss: 0.9343 - val_accuracy: 0.7228\n",
            "Epoch 42/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0206 - accuracy: 0.6903 - val_loss: 0.9259 - val_accuracy: 0.7269\n",
            "Epoch 43/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0153 - accuracy: 0.6909 - val_loss: 0.9180 - val_accuracy: 0.7273\n",
            "Epoch 44/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9953 - accuracy: 0.6943 - val_loss: 0.9101 - val_accuracy: 0.7312\n",
            "Epoch 45/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9936 - accuracy: 0.6987 - val_loss: 0.9022 - val_accuracy: 0.7356\n",
            "Epoch 46/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9874 - accuracy: 0.6986 - val_loss: 0.8932 - val_accuracy: 0.7376\n",
            "Epoch 47/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9674 - accuracy: 0.7063 - val_loss: 0.8915 - val_accuracy: 0.7356\n",
            "Epoch 48/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9675 - accuracy: 0.7058 - val_loss: 0.8807 - val_accuracy: 0.7411\n",
            "Epoch 49/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9554 - accuracy: 0.7098 - val_loss: 0.8775 - val_accuracy: 0.7394\n",
            "Epoch 50/50\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9531 - accuracy: 0.7103 - val_loss: 0.8708 - val_accuracy: 0.7376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGQY0H8Gu7nF",
        "outputId": "c6f76bdb-bb1d-4066-89f0-11b14d34482c"
      },
      "source": [
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 3s 3ms/step - loss: 0.9574 - accuracy: 0.7154\n",
            "Test loss: 0.9573673009872437\n",
            "Test accuracy: 0.715350329875946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wBKan6IVIY_"
      },
      "source": [
        "##2.2) Part 2\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmJEZrWAa5PO"
      },
      "source": [
        "When training a model, model stops generalizing after a while and learns noices of the datapoints as well. One way to avoid this **overfitting** is to use **early stopping**. With early stopping, model performance is monitored on validation data in each epoch and stops updating weights when validation performance starts decreasing.\n",
        "\n",
        "https://keras.io/api/callbacks/early_stopping/\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=1Rs8FkpVgifspzvlIfdTDYyBVjR01OgVj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6ZQh7APBmDr"
      },
      "source": [
        "### Model 11 (**Early Stopping**)\n",
        "\n",
        "Entering extreme values to observe overfitting and see (hopefully) early stopping. For some reason my previous models seemed to underfit.\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 300`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 512`\n",
        "\n",
        "<font color=red>**WITH EARLY STOPPING**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKu4TRlFgcdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af595dcc-f5ba-49a5-a55b-842eb4a62bd8"
      },
      "source": [
        "# Build & compile the model\n",
        "\n",
        "del model\n",
        "model = Sequential()\n",
        "model.add(Flatten(input_shape=(X_train.shape[1:])))\n",
        "model.add(Dense(1024, activation='relu', name='hidden_layer1'))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer2'))\n",
        "model.add(Dense(512, activation='relu', name='hidden_layer3'))\n",
        "model.add(Dense(10, activation='softmax', name='output_layer'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.00001), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_11 (Flatten)         (None, 3072)              0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 1024)              3146752   \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 3,939,338\n",
            "Trainable params: 3,939,338\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0nPbz5AVOFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5e6fb2-1ba0-4f5e-da62-8b6dab671d88"
      },
      "source": [
        "# Aim is to stop when minimum validation loss is achieved\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=10, \\\n",
        "                    verbose=1, mode='min',restore_best_weights=True) # Training will stop when minimum validation loss is achieved\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=256 ,\n",
        "                              epochs=300, validation_split = 0.2, # you can have large epoch size since it will stop when best validation loss is achieved\n",
        "                              callbacks=[es], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "229/229 [==============================] - 3s 11ms/step - loss: 2.2665 - accuracy: 0.1759 - val_loss: 2.1792 - val_accuracy: 0.1978\n",
            "Epoch 2/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.1540 - accuracy: 0.2298 - val_loss: 2.0693 - val_accuracy: 0.3041\n",
            "Epoch 3/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 2.0220 - accuracy: 0.3202 - val_loss: 1.9076 - val_accuracy: 0.3759\n",
            "Epoch 4/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.8581 - accuracy: 0.4032 - val_loss: 1.7482 - val_accuracy: 0.4384\n",
            "Epoch 5/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.6899 - accuracy: 0.4740 - val_loss: 1.6073 - val_accuracy: 0.5013\n",
            "Epoch 6/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.5567 - accuracy: 0.5280 - val_loss: 1.4981 - val_accuracy: 0.5495\n",
            "Epoch 7/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.4501 - accuracy: 0.5669 - val_loss: 1.4100 - val_accuracy: 0.5859\n",
            "Epoch 8/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3679 - accuracy: 0.5963 - val_loss: 1.3478 - val_accuracy: 0.6051\n",
            "Epoch 9/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.3106 - accuracy: 0.6145 - val_loss: 1.2925 - val_accuracy: 0.6300\n",
            "Epoch 10/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2500 - accuracy: 0.6333 - val_loss: 1.2558 - val_accuracy: 0.6194\n",
            "Epoch 11/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.2119 - accuracy: 0.6437 - val_loss: 1.2088 - val_accuracy: 0.6493\n",
            "Epoch 12/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1756 - accuracy: 0.6578 - val_loss: 1.1841 - val_accuracy: 0.6589\n",
            "Epoch 13/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1445 - accuracy: 0.6663 - val_loss: 1.1478 - val_accuracy: 0.6651\n",
            "Epoch 14/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.1186 - accuracy: 0.6721 - val_loss: 1.1266 - val_accuracy: 0.6711\n",
            "Epoch 15/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0902 - accuracy: 0.6823 - val_loss: 1.1032 - val_accuracy: 0.6796\n",
            "Epoch 16/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0604 - accuracy: 0.6877 - val_loss: 1.0828 - val_accuracy: 0.6866\n",
            "Epoch 17/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0442 - accuracy: 0.6940 - val_loss: 1.0638 - val_accuracy: 0.6931\n",
            "Epoch 18/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0230 - accuracy: 0.7033 - val_loss: 1.0568 - val_accuracy: 0.6852\n",
            "Epoch 19/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 1.0099 - accuracy: 0.7053 - val_loss: 1.0324 - val_accuracy: 0.6989\n",
            "Epoch 20/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9910 - accuracy: 0.7120 - val_loss: 1.0169 - val_accuracy: 0.7007\n",
            "Epoch 21/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9793 - accuracy: 0.7139 - val_loss: 1.0073 - val_accuracy: 0.7050\n",
            "Epoch 22/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9572 - accuracy: 0.7199 - val_loss: 0.9931 - val_accuracy: 0.7078\n",
            "Epoch 23/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9455 - accuracy: 0.7234 - val_loss: 0.9812 - val_accuracy: 0.7094\n",
            "Epoch 24/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9359 - accuracy: 0.7244 - val_loss: 0.9751 - val_accuracy: 0.7110\n",
            "Epoch 25/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9248 - accuracy: 0.7290 - val_loss: 0.9597 - val_accuracy: 0.7181\n",
            "Epoch 26/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9153 - accuracy: 0.7300 - val_loss: 0.9480 - val_accuracy: 0.7179\n",
            "Epoch 27/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.9058 - accuracy: 0.7330 - val_loss: 0.9323 - val_accuracy: 0.7273\n",
            "Epoch 28/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8898 - accuracy: 0.7401 - val_loss: 0.9343 - val_accuracy: 0.7212\n",
            "Epoch 29/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8716 - accuracy: 0.7422 - val_loss: 0.9138 - val_accuracy: 0.7278\n",
            "Epoch 30/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8638 - accuracy: 0.7473 - val_loss: 0.9106 - val_accuracy: 0.7305\n",
            "Epoch 31/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8567 - accuracy: 0.7490 - val_loss: 0.8993 - val_accuracy: 0.7315\n",
            "Epoch 32/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8424 - accuracy: 0.7520 - val_loss: 0.8889 - val_accuracy: 0.7351\n",
            "Epoch 33/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8370 - accuracy: 0.7527 - val_loss: 0.8787 - val_accuracy: 0.7399\n",
            "Epoch 34/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8214 - accuracy: 0.7600 - val_loss: 0.8732 - val_accuracy: 0.7410\n",
            "Epoch 35/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8222 - accuracy: 0.7601 - val_loss: 0.8749 - val_accuracy: 0.7415\n",
            "Epoch 36/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8116 - accuracy: 0.7608 - val_loss: 0.8591 - val_accuracy: 0.7432\n",
            "Epoch 37/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.8081 - accuracy: 0.7637 - val_loss: 0.8517 - val_accuracy: 0.7439\n",
            "Epoch 38/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7951 - accuracy: 0.7678 - val_loss: 0.8519 - val_accuracy: 0.7432\n",
            "Epoch 39/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7968 - accuracy: 0.7634 - val_loss: 0.8438 - val_accuracy: 0.7449\n",
            "Epoch 40/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7851 - accuracy: 0.7679 - val_loss: 0.8376 - val_accuracy: 0.7443\n",
            "Epoch 41/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7729 - accuracy: 0.7717 - val_loss: 0.8276 - val_accuracy: 0.7525\n",
            "Epoch 42/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7562 - accuracy: 0.7802 - val_loss: 0.8213 - val_accuracy: 0.7569\n",
            "Epoch 43/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7547 - accuracy: 0.7797 - val_loss: 0.8180 - val_accuracy: 0.7579\n",
            "Epoch 44/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7579 - accuracy: 0.7772 - val_loss: 0.8160 - val_accuracy: 0.7584\n",
            "Epoch 45/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7538 - accuracy: 0.7780 - val_loss: 0.8011 - val_accuracy: 0.7594\n",
            "Epoch 46/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7401 - accuracy: 0.7845 - val_loss: 0.8085 - val_accuracy: 0.7573\n",
            "Epoch 47/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7388 - accuracy: 0.7820 - val_loss: 0.7993 - val_accuracy: 0.7606\n",
            "Epoch 48/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7260 - accuracy: 0.7869 - val_loss: 0.7862 - val_accuracy: 0.7672\n",
            "Epoch 49/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7225 - accuracy: 0.7881 - val_loss: 0.7823 - val_accuracy: 0.7688\n",
            "Epoch 50/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7235 - accuracy: 0.7861 - val_loss: 0.7783 - val_accuracy: 0.7700\n",
            "Epoch 51/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7121 - accuracy: 0.7901 - val_loss: 0.7886 - val_accuracy: 0.7604\n",
            "Epoch 52/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.7068 - accuracy: 0.7925 - val_loss: 0.7697 - val_accuracy: 0.7716\n",
            "Epoch 53/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6950 - accuracy: 0.7960 - val_loss: 0.7632 - val_accuracy: 0.7723\n",
            "Epoch 54/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6922 - accuracy: 0.7970 - val_loss: 0.7592 - val_accuracy: 0.7732\n",
            "Epoch 55/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6904 - accuracy: 0.7966 - val_loss: 0.7518 - val_accuracy: 0.7770\n",
            "Epoch 56/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6912 - accuracy: 0.7979 - val_loss: 0.7529 - val_accuracy: 0.7751\n",
            "Epoch 57/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6827 - accuracy: 0.8000 - val_loss: 0.7517 - val_accuracy: 0.7777\n",
            "Epoch 58/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6679 - accuracy: 0.8039 - val_loss: 0.7433 - val_accuracy: 0.7799\n",
            "Epoch 59/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6676 - accuracy: 0.8037 - val_loss: 0.7460 - val_accuracy: 0.7810\n",
            "Epoch 60/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6668 - accuracy: 0.8046 - val_loss: 0.7384 - val_accuracy: 0.7842\n",
            "Epoch 61/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6573 - accuracy: 0.8075 - val_loss: 0.7293 - val_accuracy: 0.7858\n",
            "Epoch 62/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6593 - accuracy: 0.8061 - val_loss: 0.7364 - val_accuracy: 0.7813\n",
            "Epoch 63/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6499 - accuracy: 0.8090 - val_loss: 0.7260 - val_accuracy: 0.7839\n",
            "Epoch 64/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6486 - accuracy: 0.8102 - val_loss: 0.7246 - val_accuracy: 0.7855\n",
            "Epoch 65/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6374 - accuracy: 0.8126 - val_loss: 0.7263 - val_accuracy: 0.7820\n",
            "Epoch 66/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6451 - accuracy: 0.8117 - val_loss: 0.7267 - val_accuracy: 0.7810\n",
            "Epoch 67/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6394 - accuracy: 0.8123 - val_loss: 0.7118 - val_accuracy: 0.7918\n",
            "Epoch 68/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6259 - accuracy: 0.8182 - val_loss: 0.7324 - val_accuracy: 0.7851\n",
            "Epoch 69/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6334 - accuracy: 0.8167 - val_loss: 0.7082 - val_accuracy: 0.7905\n",
            "Epoch 70/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6125 - accuracy: 0.8197 - val_loss: 0.7064 - val_accuracy: 0.7905\n",
            "Epoch 71/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6203 - accuracy: 0.8187 - val_loss: 0.7022 - val_accuracy: 0.7935\n",
            "Epoch 72/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6174 - accuracy: 0.8176 - val_loss: 0.7028 - val_accuracy: 0.7937\n",
            "Epoch 73/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6166 - accuracy: 0.8193 - val_loss: 0.6985 - val_accuracy: 0.7976\n",
            "Epoch 74/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6087 - accuracy: 0.8232 - val_loss: 0.6950 - val_accuracy: 0.7977\n",
            "Epoch 75/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5981 - accuracy: 0.8235 - val_loss: 0.6870 - val_accuracy: 0.7997\n",
            "Epoch 76/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.6053 - accuracy: 0.8218 - val_loss: 0.6899 - val_accuracy: 0.7956\n",
            "Epoch 77/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5928 - accuracy: 0.8255 - val_loss: 0.6845 - val_accuracy: 0.7994\n",
            "Epoch 78/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5944 - accuracy: 0.8267 - val_loss: 0.6854 - val_accuracy: 0.8002\n",
            "Epoch 79/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5924 - accuracy: 0.8264 - val_loss: 0.6784 - val_accuracy: 0.8008\n",
            "Epoch 80/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5836 - accuracy: 0.8296 - val_loss: 0.6832 - val_accuracy: 0.7981\n",
            "Epoch 81/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5865 - accuracy: 0.8301 - val_loss: 0.6690 - val_accuracy: 0.8058\n",
            "Epoch 82/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5760 - accuracy: 0.8311 - val_loss: 0.6733 - val_accuracy: 0.8021\n",
            "Epoch 83/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5725 - accuracy: 0.8328 - val_loss: 0.6751 - val_accuracy: 0.8004\n",
            "Epoch 84/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5744 - accuracy: 0.8320 - val_loss: 0.6674 - val_accuracy: 0.8068\n",
            "Epoch 85/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5684 - accuracy: 0.8324 - val_loss: 0.6657 - val_accuracy: 0.8076\n",
            "Epoch 86/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5601 - accuracy: 0.8370 - val_loss: 0.6696 - val_accuracy: 0.8038\n",
            "Epoch 87/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5609 - accuracy: 0.8366 - val_loss: 0.6603 - val_accuracy: 0.8075\n",
            "Epoch 88/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5536 - accuracy: 0.8381 - val_loss: 0.6630 - val_accuracy: 0.8072\n",
            "Epoch 89/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5588 - accuracy: 0.8354 - val_loss: 0.6618 - val_accuracy: 0.8069\n",
            "Epoch 90/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5532 - accuracy: 0.8381 - val_loss: 0.6559 - val_accuracy: 0.8080\n",
            "Epoch 91/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5497 - accuracy: 0.8399 - val_loss: 0.6536 - val_accuracy: 0.8087\n",
            "Epoch 92/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5490 - accuracy: 0.8404 - val_loss: 0.6596 - val_accuracy: 0.8085\n",
            "Epoch 93/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5547 - accuracy: 0.8391 - val_loss: 0.6519 - val_accuracy: 0.8102\n",
            "Epoch 94/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5438 - accuracy: 0.8391 - val_loss: 0.6455 - val_accuracy: 0.8105\n",
            "Epoch 95/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5404 - accuracy: 0.8415 - val_loss: 0.6494 - val_accuracy: 0.8084\n",
            "Epoch 96/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5404 - accuracy: 0.8416 - val_loss: 0.6429 - val_accuracy: 0.8120\n",
            "Epoch 97/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5289 - accuracy: 0.8454 - val_loss: 0.6411 - val_accuracy: 0.8137\n",
            "Epoch 98/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5378 - accuracy: 0.8415 - val_loss: 0.6463 - val_accuracy: 0.8150\n",
            "Epoch 99/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5336 - accuracy: 0.8427 - val_loss: 0.6359 - val_accuracy: 0.8145\n",
            "Epoch 100/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5232 - accuracy: 0.8487 - val_loss: 0.6440 - val_accuracy: 0.8109\n",
            "Epoch 101/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5167 - accuracy: 0.8500 - val_loss: 0.6371 - val_accuracy: 0.8136\n",
            "Epoch 102/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5223 - accuracy: 0.8480 - val_loss: 0.6313 - val_accuracy: 0.8170\n",
            "Epoch 103/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5211 - accuracy: 0.8483 - val_loss: 0.6313 - val_accuracy: 0.8162\n",
            "Epoch 104/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5199 - accuracy: 0.8494 - val_loss: 0.6337 - val_accuracy: 0.8146\n",
            "Epoch 105/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5156 - accuracy: 0.8486 - val_loss: 0.6365 - val_accuracy: 0.8135\n",
            "Epoch 106/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5035 - accuracy: 0.8539 - val_loss: 0.6348 - val_accuracy: 0.8137\n",
            "Epoch 107/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5131 - accuracy: 0.8511 - val_loss: 0.6244 - val_accuracy: 0.8212\n",
            "Epoch 108/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5062 - accuracy: 0.8539 - val_loss: 0.6388 - val_accuracy: 0.8157\n",
            "Epoch 109/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4996 - accuracy: 0.8542 - val_loss: 0.6245 - val_accuracy: 0.8195\n",
            "Epoch 110/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.5008 - accuracy: 0.8536 - val_loss: 0.6212 - val_accuracy: 0.8198\n",
            "Epoch 111/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4996 - accuracy: 0.8550 - val_loss: 0.6184 - val_accuracy: 0.8215\n",
            "Epoch 112/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4971 - accuracy: 0.8569 - val_loss: 0.6142 - val_accuracy: 0.8232\n",
            "Epoch 113/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4865 - accuracy: 0.8576 - val_loss: 0.6165 - val_accuracy: 0.8212\n",
            "Epoch 114/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4899 - accuracy: 0.8591 - val_loss: 0.6234 - val_accuracy: 0.8206\n",
            "Epoch 115/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4894 - accuracy: 0.8578 - val_loss: 0.6205 - val_accuracy: 0.8210\n",
            "Epoch 116/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4903 - accuracy: 0.8576 - val_loss: 0.6121 - val_accuracy: 0.8228\n",
            "Epoch 117/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4794 - accuracy: 0.8613 - val_loss: 0.6264 - val_accuracy: 0.8186\n",
            "Epoch 118/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4826 - accuracy: 0.8599 - val_loss: 0.6153 - val_accuracy: 0.8215\n",
            "Epoch 119/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4802 - accuracy: 0.8602 - val_loss: 0.6035 - val_accuracy: 0.8279\n",
            "Epoch 120/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4831 - accuracy: 0.8607 - val_loss: 0.6133 - val_accuracy: 0.8202\n",
            "Epoch 121/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4809 - accuracy: 0.8613 - val_loss: 0.6012 - val_accuracy: 0.8258\n",
            "Epoch 122/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4681 - accuracy: 0.8646 - val_loss: 0.6017 - val_accuracy: 0.8281\n",
            "Epoch 123/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4712 - accuracy: 0.8645 - val_loss: 0.6006 - val_accuracy: 0.8281\n",
            "Epoch 124/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4680 - accuracy: 0.8655 - val_loss: 0.6066 - val_accuracy: 0.8256\n",
            "Epoch 125/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4598 - accuracy: 0.8675 - val_loss: 0.6073 - val_accuracy: 0.8235\n",
            "Epoch 126/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4583 - accuracy: 0.8677 - val_loss: 0.5954 - val_accuracy: 0.8291\n",
            "Epoch 127/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4633 - accuracy: 0.8670 - val_loss: 0.5983 - val_accuracy: 0.8281\n",
            "Epoch 128/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4583 - accuracy: 0.8669 - val_loss: 0.5953 - val_accuracy: 0.8277\n",
            "Epoch 129/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4574 - accuracy: 0.8683 - val_loss: 0.6051 - val_accuracy: 0.8256\n",
            "Epoch 130/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4567 - accuracy: 0.8671 - val_loss: 0.5941 - val_accuracy: 0.8318\n",
            "Epoch 131/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4489 - accuracy: 0.8702 - val_loss: 0.5976 - val_accuracy: 0.8286\n",
            "Epoch 132/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4475 - accuracy: 0.8711 - val_loss: 0.5955 - val_accuracy: 0.8292\n",
            "Epoch 133/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4504 - accuracy: 0.8688 - val_loss: 0.5881 - val_accuracy: 0.8325\n",
            "Epoch 134/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4419 - accuracy: 0.8722 - val_loss: 0.5887 - val_accuracy: 0.8305\n",
            "Epoch 135/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4442 - accuracy: 0.8713 - val_loss: 0.5956 - val_accuracy: 0.8299\n",
            "Epoch 136/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4414 - accuracy: 0.8706 - val_loss: 0.5953 - val_accuracy: 0.8267\n",
            "Epoch 137/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4398 - accuracy: 0.8719 - val_loss: 0.5852 - val_accuracy: 0.8336\n",
            "Epoch 138/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4355 - accuracy: 0.8738 - val_loss: 0.5938 - val_accuracy: 0.8274\n",
            "Epoch 139/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4385 - accuracy: 0.8715 - val_loss: 0.5845 - val_accuracy: 0.8332\n",
            "Epoch 140/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4362 - accuracy: 0.8757 - val_loss: 0.5881 - val_accuracy: 0.8318\n",
            "Epoch 141/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4337 - accuracy: 0.8737 - val_loss: 0.5865 - val_accuracy: 0.8298\n",
            "Epoch 142/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4264 - accuracy: 0.8782 - val_loss: 0.5881 - val_accuracy: 0.8327\n",
            "Epoch 143/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4256 - accuracy: 0.8777 - val_loss: 0.5811 - val_accuracy: 0.8329\n",
            "Epoch 144/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4268 - accuracy: 0.8762 - val_loss: 0.5889 - val_accuracy: 0.8296\n",
            "Epoch 145/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4262 - accuracy: 0.8766 - val_loss: 0.5913 - val_accuracy: 0.8330\n",
            "Epoch 146/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4241 - accuracy: 0.8777 - val_loss: 0.5861 - val_accuracy: 0.8335\n",
            "Epoch 147/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4284 - accuracy: 0.8771 - val_loss: 0.5827 - val_accuracy: 0.8337\n",
            "Epoch 148/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4207 - accuracy: 0.8799 - val_loss: 0.5805 - val_accuracy: 0.8351\n",
            "Epoch 149/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4246 - accuracy: 0.8778 - val_loss: 0.5795 - val_accuracy: 0.8310\n",
            "Epoch 150/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4119 - accuracy: 0.8831 - val_loss: 0.5799 - val_accuracy: 0.8351\n",
            "Epoch 151/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4078 - accuracy: 0.8833 - val_loss: 0.5786 - val_accuracy: 0.8338\n",
            "Epoch 152/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4151 - accuracy: 0.8814 - val_loss: 0.5791 - val_accuracy: 0.8320\n",
            "Epoch 153/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4172 - accuracy: 0.8789 - val_loss: 0.5791 - val_accuracy: 0.8371\n",
            "Epoch 154/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4084 - accuracy: 0.8812 - val_loss: 0.5811 - val_accuracy: 0.8341\n",
            "Epoch 155/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4130 - accuracy: 0.8822 - val_loss: 0.5728 - val_accuracy: 0.8359\n",
            "Epoch 156/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4093 - accuracy: 0.8834 - val_loss: 0.5738 - val_accuracy: 0.8352\n",
            "Epoch 157/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4052 - accuracy: 0.8813 - val_loss: 0.5767 - val_accuracy: 0.8358\n",
            "Epoch 158/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4059 - accuracy: 0.8825 - val_loss: 0.5853 - val_accuracy: 0.8311\n",
            "Epoch 159/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4025 - accuracy: 0.8846 - val_loss: 0.5704 - val_accuracy: 0.8356\n",
            "Epoch 160/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3985 - accuracy: 0.8843 - val_loss: 0.5781 - val_accuracy: 0.8378\n",
            "Epoch 161/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.4004 - accuracy: 0.8859 - val_loss: 0.5840 - val_accuracy: 0.8354\n",
            "Epoch 162/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3968 - accuracy: 0.8863 - val_loss: 0.5777 - val_accuracy: 0.8365\n",
            "Epoch 163/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3955 - accuracy: 0.8861 - val_loss: 0.5797 - val_accuracy: 0.8349\n",
            "Epoch 164/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3891 - accuracy: 0.8890 - val_loss: 0.5735 - val_accuracy: 0.8380\n",
            "Epoch 165/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3859 - accuracy: 0.8889 - val_loss: 0.5755 - val_accuracy: 0.8345\n",
            "Epoch 166/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3936 - accuracy: 0.8869 - val_loss: 0.5734 - val_accuracy: 0.8347\n",
            "Epoch 167/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3890 - accuracy: 0.8887 - val_loss: 0.5707 - val_accuracy: 0.8359\n",
            "Epoch 168/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3847 - accuracy: 0.8899 - val_loss: 0.5710 - val_accuracy: 0.8359\n",
            "Epoch 169/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3870 - accuracy: 0.8885 - val_loss: 0.5667 - val_accuracy: 0.8381\n",
            "Epoch 170/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3856 - accuracy: 0.8884 - val_loss: 0.5665 - val_accuracy: 0.8397\n",
            "Epoch 171/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3884 - accuracy: 0.8886 - val_loss: 0.5620 - val_accuracy: 0.8403\n",
            "Epoch 172/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3769 - accuracy: 0.8905 - val_loss: 0.5639 - val_accuracy: 0.8406\n",
            "Epoch 173/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3787 - accuracy: 0.8910 - val_loss: 0.5683 - val_accuracy: 0.8378\n",
            "Epoch 174/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3784 - accuracy: 0.8911 - val_loss: 0.5661 - val_accuracy: 0.8395\n",
            "Epoch 175/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3713 - accuracy: 0.8948 - val_loss: 0.5636 - val_accuracy: 0.8421\n",
            "Epoch 176/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3758 - accuracy: 0.8900 - val_loss: 0.5573 - val_accuracy: 0.8451\n",
            "Epoch 177/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3706 - accuracy: 0.8945 - val_loss: 0.5596 - val_accuracy: 0.8424\n",
            "Epoch 178/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3710 - accuracy: 0.8937 - val_loss: 0.5597 - val_accuracy: 0.8434\n",
            "Epoch 179/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3753 - accuracy: 0.8929 - val_loss: 0.5765 - val_accuracy: 0.8357\n",
            "Epoch 180/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3664 - accuracy: 0.8946 - val_loss: 0.5670 - val_accuracy: 0.8413\n",
            "Epoch 181/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3642 - accuracy: 0.8963 - val_loss: 0.5847 - val_accuracy: 0.8340\n",
            "Epoch 182/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3673 - accuracy: 0.8932 - val_loss: 0.5688 - val_accuracy: 0.8413\n",
            "Epoch 183/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3673 - accuracy: 0.8951 - val_loss: 0.5652 - val_accuracy: 0.8417\n",
            "Epoch 184/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3606 - accuracy: 0.8951 - val_loss: 0.5714 - val_accuracy: 0.8413\n",
            "Epoch 185/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3616 - accuracy: 0.8934 - val_loss: 0.5619 - val_accuracy: 0.8415\n",
            "Epoch 186/300\n",
            "229/229 [==============================] - 2s 9ms/step - loss: 0.3549 - accuracy: 0.8980 - val_loss: 0.5591 - val_accuracy: 0.8436\n",
            "Restoring model weights from the end of the best epoch.\n",
            "Epoch 00186: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fe420262550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRF2weImf12m"
      },
      "source": [
        "![picture](https://drive.google.com/uc?id=1Mp2W2VVQVmyuleSDNDKJLI6hxAtyG5gx)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnfijwFjePc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "75074528-6888-404a-a341-efed5dc47aff"
      },
      "source": [
        "#Plot train & validation loss\n",
        "\n",
        "plt.plot(model.history.history['loss'])\n",
        "plt.plot(model.history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoBW8oIBgZfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa36e55-8c2e-4463-bb43-4779236daadf"
      },
      "source": [
        "#Evaluate on test data\n",
        "\n",
        "score = model.evaluate(X_test, y_test)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "814/814 [==============================] - 3s 3ms/step - loss: 0.6639 - accuracy: 0.8223\n",
            "Test loss: 0.6639110445976257\n",
            "Test accuracy: 0.8223340511322021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNzYiVX5UyI-"
      },
      "source": [
        "#3) Report\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Summary of models you try (learning rate, # of hidden layers, # of epochs, test accuracy, etc.) Write a short report & discuss the results of the models you trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAGFVoVifKe"
      },
      "source": [
        "##Summary of the Models\n",
        "\n",
        "---\n",
        "\n",
        "**Model 1**\n",
        "\n",
        "`learning_rate: 0.1`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.19587430357933044</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 2**\n",
        "\n",
        "Comparing performance according to <font color=red>`learning_rate`</font> with *Model 1*\n",
        "\n",
        "`learning_rate: 0.00001` changed from <font color=red>`0.1`</font>\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.6793946027755737</font>\n",
        "\n",
        "---\n",
        "**Model 3**\n",
        "\n",
        "Comparing performance according to <font color=red>`epoch`</font> with *Model 2*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 100` changed from <font color=red>`50`</font>\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.7339044213294983</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 4**\n",
        "\n",
        "Comparing performance according to <font color = red>`epoch`</font> with *Model 2* and *Model 3*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 200` changed from <font color=red>`50 and 100`</font>\n",
        "\n",
        "`# of hidden layers: 2`\n",
        "\n",
        "`# of neurons: 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.7906038761138916</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 5**\n",
        "\n",
        "Comparing performance according to <font color = red>`# of hidden layers`</font> with *Model 2*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3` changed from <font color=red>`2`</font>\n",
        "\n",
        "`# of neurons: 1024, 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.7272971868515015</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 6**\n",
        "\n",
        "Comparing performance according to <font color=red>`# of hidden layers`</font> with *Model 2* and *Model 5*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 1` changed from <font color=red>`2 and 3`</font>\n",
        "\n",
        "`# of neurons: 512`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.6484711170196533</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 7**\n",
        "\n",
        "Comparing performance according to <font color=red>`# of neurons`</font> with *Model 5*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 256, 64, 16` changed from <font color=red>`1024, 512, 128`</font>\n",
        "\n",
        "<font color=blue>Test accuracy: 0.5524354577064514</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 8**\n",
        "\n",
        "Comparing performance according to <font color=red>`# of neurons`</font> with *Model 5 and Model 7*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 50`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 512, 128, 64` changed from <font color=red>`1024, 512, 128 and 256, 64, 16`</font>\n",
        "\n",
        "<font color=blue>Test accuracy: 0.6773202419281006</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 9**\n",
        "\n",
        "Trying to guess best performing hyperparameters from previous comparions without paying attention to time and GPU power/RAM used.\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 200`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 128`\n",
        "\n",
        "<font color=blue>Test accuracy: 0.8170713186264038</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 10 (**Dropout**)**\n",
        "\n",
        "Comparing performance according to <font color = red>`regularization (dropout)`</font> with *Model 3*\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 100`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 128`\n",
        "\n",
        "<font color=red>**WITH DROPOUT**</font>\n",
        "\n",
        "<font color=blue>Test accuracy: 0.715350329875946</font>\n",
        "\n",
        "---\n",
        "\n",
        "**Model 11 (**Early Stopping**)**\n",
        "\n",
        "Entering extreme values to observe overfitting and see (hopefully) early stopping. For some reason my previous models seemed to underfit.\n",
        "\n",
        "`learning_rate: 0.00001`\n",
        "\n",
        "`epoch: 300`\n",
        "\n",
        "`# of hidden layers: 3`\n",
        "\n",
        "`# of neurons: 1024, 512, 512`\n",
        "\n",
        "<font color=red>**WITH EARLY STOPPING**</font>\n",
        "\n",
        "<font color=blue>Test accuracy: 0.8223340511322021</font>\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3suULROIKBj"
      },
      "source": [
        "**Problem Definition**\n",
        "\n",
        "> In this homework, our task was to train multilayer perceptron models for classifying SVHN dataset and understand how certain hyperparameters affect the test accuracy score, and what can be done to combat overfitting.\n",
        "\n",
        "**Preprocessing**\n",
        "\n",
        "> For preprocessing, after loading the train and test sets; changed the input shapes, normalized the X_train and X_test, replaced '10's in y_train and y_test with '0's, and one-hot encoded the labels as setting `loss='categorical_crossentropy'` meant that I needed one-hot encoded labels.\n",
        "\n",
        "**How did you choose the best hyperparameters?**\n",
        "> I chose the \"best\" hyperparameters according to the validation accuracy score without paying much attention to the time it takes to train the model and GPU power/RAM, and storage used as my models. Most of my models seemed to underfit (even the slightest increase in model complexity improved test accuracy scores) with the given hyperparameters in the document. However, one must also pay attention to to the time it takes to train the model and GPU power/RAM, and storage used.\n",
        "\n",
        "**What happened when the # of epochs are too large/small, why?**\n",
        "> As seen from Model 2, Model 3 and Model 4, as `# of epochs` increases, the validation and test accuracy seemed to increase. This was no surprise because more iterations through the training set means more learning is done, which (generally) leads to higher accuracy scores. Too small `# of epochs` will lead to underfitting and too large `# of epochs` will lead to overfitting.\n",
        "\n",
        "**What happened when the learning rate is too large/small, why?**\n",
        "> As seen from Model 1 and Model 2, by setting `learning_rate` to 0.1, we got a extremely simple (and inaccurate) model with a test accuracy score of 0.19587430357933044. But when we set the `learning_rate` to 0.00001, test accuracy score increased by 240%. When the learning rate was very high (0.1), gradient descent failed to converge to local minima. Too large `learning_rate` will lead to underfitting and large loss.\n",
        "\n",
        "**What did you observe when you change the # of hidden layers?**\n",
        "> Increasing `# of hidden layers` meant increasing model complexity, which increased validation and test accuracy scores as seen by Model 2, Model 5 and Model 6. \n",
        "\n",
        "**What did you observe when you change the # of neurons?**\n",
        ">  Increasing `# of neurons` meant increasing model complexity, which increased validation and test accuracy scores as seen by Model 5, Model 7 and Model 8. \n",
        "\n",
        "**What is the use of adding dropout?**\n",
        "> It is a regularization approach used to prevent overfitting.\n",
        "\n",
        "**What is the use of early stopping?**\n",
        "> We use early stopping approach to prevent overfitting by stopping updating weights when validation accuracy score stops improving.\n"
      ]
    }
  ]
}